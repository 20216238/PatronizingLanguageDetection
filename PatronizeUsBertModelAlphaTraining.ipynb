{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed9d07e",
   "metadata": {},
   "source": [
    "# Semeval offense notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a460d91",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be05977",
   "metadata": {},
   "source": [
    "The tasks were given by\n",
    "Sub task 1- offensive language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce04092",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ef9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '.\\inputDir\\ref'\n",
    "\n",
    "SAVE_PATH = './save'\n",
    "\n",
    "TRAIN_PATH = './inputDir/ref/dontpatronizeme_pcl.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815eaa30",
   "metadata": {},
   "source": [
    "# Function Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81b5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import wordsegment\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "import copy\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "# Local files\n",
    "#python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ac621",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a18c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(toSave, filename, mode='wb'):\n",
    "    dirname = os.path.dirname(filename)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    file = open(filename, mode)\n",
    "    pickle.dump(toSave, file)\n",
    "    file.close()\n",
    "\n",
    "def load(filename, mode='rb'):\n",
    "    file = open(filename, mode)\n",
    "    loaded = pickle.load(file)\n",
    "    file.close()\n",
    "    return loaded\n",
    "\n",
    "def tokenizationFunction(sents, pad_token):\n",
    "    sents_padded = []\n",
    "    lens = lensFinderFunction(sents)\n",
    "    max_len = max(lens)\n",
    "    sents_padded = [sents[i] + [pad_token] * (max_len - l) for i, l in enumerate(lens)]\n",
    "    return sents_padded\n",
    "\n",
    "def sortingFunction(sents, reverse=True):\n",
    "    sents.sort(key=(lambda s: len(s)), reverse=reverse)\n",
    "    return sents\n",
    "\n",
    "def maskFinderFunction(sents, unmask_idx=1, mask_idx=0):\n",
    "    lens = lensFinderFunction(sents)\n",
    "    max_len = max(lens)\n",
    "    mask = [([unmask_idx] * l + [mask_idx] * (max_len - l)) for l in lens]\n",
    "    return mask\n",
    "\n",
    "def lensFinderFunction(sents):\n",
    "    return [len(sent) for sent in sents]\n",
    "\n",
    "#def getMaskLength(sents):\n",
    "#    max_len = max([len(sent) for sent in sents])\n",
    "#    return max_len\n",
    "\n",
    "#def truncateLengthArray(sents, length):\n",
    "#    sents = [sent[:length] for sent in sents]\n",
    "#    return sents\n",
    "\n",
    "def get_loss_weight(labels, label_order):\n",
    "    nums = [np.sum(labels == lo) for lo in label_order]\n",
    "    loss_weight = torch.tensor([n / len(labels) for n in nums])\n",
    "    return loss_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dac77",
   "metadata": {},
   "source": [
    "# Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87241ec1",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb487115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm = DontPatronizeMe('.', 'dontpatronizeme_pcl.tsv')\n",
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "\n",
    "# which we can then access as a dataframe\n",
    "dpm.train_task1_df.head()\n",
    "\n",
    "data=dpm.train_task1_df\n",
    "trainData, testData = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "wordsegment.load()\n",
    "\n",
    "\n",
    "def readPatronizationFile(filepath: str):\n",
    "    ids = np.array(trainData['ids'].values)\n",
    "    text = np.array(trainData['text'].values)\n",
    "    label_1 = np.array(trainData['labels'].values)\n",
    "    \n",
    "    \n",
    "    # Process text\n",
    "    text = textProcessingFunction(text)\n",
    "    nums = len(trainData)\n",
    "    return ids,nums, text, label_1# title#, label_b, label_c\n",
    "\n",
    "\n",
    "\n",
    "def testDataCreationFunction(task, tokenizer, truncate=512):\n",
    "    ids = np.array(testData['ids'].values)\n",
    "    texts = np.array(testData['text'].values)\n",
    "    label_1 = np.array(testData['labels'].values)\n",
    "    \n",
    "    # Process text\n",
    "    texts = textProcessingFunction(texts)\n",
    "    nums = len(testData)\n",
    "    token_ids = [tokenizer.encode(text=texts[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_1\n",
    "\n",
    "def textProcessingFunction(textLines):\n",
    "    # Process textLines\n",
    "    #textLines = emoji2word(textLines)\n",
    "    #textLines = replace_rare_words(textLines)\n",
    "    textLines = remove_replicates(textLines)\n",
    "    #textLines = segment_hashtag(textLines)\n",
    "    textLines = remove_useless_punctuation(textLines)\n",
    "    textLines = np.array(textLines)\n",
    "    return textLines\n",
    "\n",
    "#def emoji2word(sents):\n",
    "#    return [emoji.demojize(sent) for sent in sents]\n",
    "\n",
    "def remove_useless_punctuation(sents):\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent = sent.replace(':', ' ')\n",
    "        sent = sent.replace('_', ' ')\n",
    "        sent = sent.replace('...', ' ')\n",
    "        sents[i] = sent\n",
    "    return sents\n",
    "\n",
    "def remove_replicates(sents):\n",
    "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
    "    # because some textLines contain so many `@USER` which may cause redundant\n",
    "    for i, sent in enumerate(sents):\n",
    "        if sent.find('@USER') != sent.rfind('@USER'):\n",
    "            sents[i] = sent.replace('@USER', '')\n",
    "            sents[i] = '@USERS ' + sents[i]\n",
    "    return sents\n",
    "\n",
    "def all_tasks(filepath: str, tokenizer, truncate=512):\n",
    "    nums, ids, textLines, label_a = readPatronizationFile(filepath)#''', label_b, label_c'''\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_a, label_b, label_c\n",
    "\n",
    "#Above will need to be redifined\n",
    "def task_1(filepath: str, tokenizer, truncate=512):\n",
    "    ids,nums, textLines, label_1= readPatronizationFile(filepath)\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_1\n",
    "\n",
    "#Below willl need to be fixed up with the other path\n",
    "def task_2(filepath: str, tokenizer, truncate=512):\n",
    "    ids, textLines, label_b,  = readPatronizationFile(filepath)\n",
    "    # Only part of the textLines are useful for task b\n",
    "\n",
    "    useful = label_b != 'NULL'\n",
    "    ids = ids[useful]\n",
    "    textLines = textLines[useful]\n",
    "    label_b = label_b[useful]\n",
    "\n",
    "    nums = len(label_b)\n",
    "    # Tokenize\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    # Get mask\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    # Get lengths\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    # Pad tokens\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac605a4",
   "metadata": {},
   "source": [
    "# Datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf21526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class PatronizationDataset(Dataset):\n",
    "    def __init__(self, input_ids, lens, mask, labels, task):\n",
    "        self.input_ids = torch.tensor(input_ids)\n",
    "        self.lens = lens\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #this_LABEL_DICT = LABEL_DICT[self.task]\n",
    "        input = self.input_ids[idx]\n",
    "        length = self.lens[idx]\n",
    "        mask = self.mask[idx]\n",
    "        #label = torch.tensor(this_LABEL_DICT[self.labels[idx]])\n",
    "        label = self.labels[idx]\n",
    "        return input, length, mask, label\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset.labels))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset.labels[id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2077ca",
   "metadata": {},
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b310bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    '''\n",
    "    The trainer for training models.\n",
    "    It can be used for both single and multi task training.\n",
    "    Every class function ends with _m is for multi-task training.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        modelNumber: nn.Module,\n",
    "        epochNumber: int,\n",
    "        dataloaderNumber: Dict[str, DataLoader],\n",
    "        criterionNumber: nn.Module,\n",
    "        loss_weightNumber: List[float],\n",
    "        clipNumber: bool,\n",
    "        optimizerNumber: torch.optim.Optimizer,\n",
    "        schedulerNumber: torch.optim.lr_scheduler,\n",
    "        deviceNumber: str,\n",
    "        patienceNumber: int,\n",
    "        taskName: str,\n",
    "        modelName: str,\n",
    "        seedNumber: int\n",
    "    ):\n",
    "        self.model = modelNumber\n",
    "        self.epochs = epochNumber\n",
    "        self.dataloaders = dataloaderNumber\n",
    "        self.criterion = criterionNumber\n",
    "        self.loss_weights = loss_weightNumber\n",
    "        self.clip = clipNumber\n",
    "        self.optimizer = optimizerNumber\n",
    "        self.scheduler = schedulerNumber\n",
    "        self.device = deviceNumber\n",
    "        self.patience = patienceNumber\n",
    "        self.task_name = taskName\n",
    "        self.model_name = modelName\n",
    "        self.seed = seedNumber\n",
    "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
    "\n",
    "        # Evaluation results\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_f1 = []\n",
    "        self.test_f1 = []\n",
    "        self.best_train_f1 = 0.0\n",
    "        self.best_test_f1 = 0.0\n",
    "\n",
    "        # Evaluation results for multi-task\n",
    "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch number {epoch}')\n",
    "            print('=' * 20)\n",
    "            print('/' * 10,'\\\\'*10)\n",
    "            self.trainSingleEpoch()\n",
    "            self.test()\n",
    "            print(f'Best test f1: {self.bestTestF1Score:.4f}')\n",
    "            print('\\\\'*10,'/' * 10)\n",
    "            print('=' * 20)\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.trainLosses, self.testingLosses, self.train_f1, self.testF1Score, self.best_train_f1, self.bestTestF1Score),\n",
    "            f'./save/results/single_{self.task_name}_{self.dateTimeString}_{self.bestTestF1Score:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def trainSingleEpoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        accuracy = accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        balancedAccuracy = balanced_accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        roc = roc_auc_score(allLabelsFound, yPredictedValue)\n",
    "\n",
    "        #TODO insert other details here\n",
    "        \n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "        print(f'Accuracy = {accuracy:.4f}')\n",
    "        print(f'Balanced Accuracy  = {balancedAccuracy:.4f}')\n",
    "        print(f'ROC value = {roc:.4f}')\n",
    "\n",
    "        self.trainLosses.append(loss)\n",
    "        self.train_f1.append(f1)\n",
    "        if f1 > self.best_train_f1:\n",
    "            self.bestTrainF1ScoreFound= f1\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                loss += _loss.item()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        accuracy = accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        balancedAccuracy = balanced_accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        roc = roc_auc_score(allLabelsFound, yPredictedValue)\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "        print(f'Accuracy = {accuracy:.4f}')\n",
    "        print(f'Balanced Accuracy  = {balancedAccuracy:.4f}')\n",
    "        print(f'ROC value = {roc:.4f}')\n",
    "        #TODO Insert other details here\n",
    "        self.testingLosses.append(loss)\n",
    "        self.testF1Score.append(f1)\n",
    "        if f1 > self.bestTestF1Score:\n",
    "            self.bestTestF1Score = f1\n",
    "            self.save_model()\n",
    "\n",
    "    def train_m(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            print('/' * 10,'\\\\'*10)\n",
    "            self.trainSingleEpoch_m()\n",
    "            self.test_m()\n",
    "            print(f'Best test results A: {self.bestTestF1Score_m[0]:.4f}')\n",
    "            print(f'Best test results B: {self.bestTestF1Score_m[1]:.4f}')\n",
    "            print(f'Best test results C: {self.bestTestF1Score_m[2]:.4f}')\n",
    "            print('=' * 20)\n",
    "            print('\\\\'*10,'/' * 10)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.trainLosses, self.testingLosses, self.train_f1, self.testF1Score, self.best_train_f1_m, self.bestTestF1Score_m),\n",
    "            f'./save/results/mtl_{self.dateTimeString}_{self.bestTestF1Score_m[0]:.4f}.pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d123cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "\n",
    "#from utils import save\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class Trainer():\n",
    "    '''\n",
    "    The trainer for training models.\n",
    "    It can be used for both single and multi task training.\n",
    "    Every class function ends with _m is for multi-task training.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        epochs: int,\n",
    "        dataloaders: Dict[str, DataLoader],\n",
    "        criterion: nn.Module,\n",
    "        loss_weights: List[float],\n",
    "        clip: bool,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        device: str,\n",
    "        patience: int,\n",
    "        task_name: str,\n",
    "        model_name: str,\n",
    "        seed: int\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        self.criterion = criterion\n",
    "        self.loss_weights = loss_weights\n",
    "        self.clip = clip\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.task_name = task_name\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
    "\n",
    "        # Evaluation results\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_f1 = []\n",
    "        self.test_f1 = []\n",
    "        self.best_train_f1 = 0.0\n",
    "        self.best_test_f1 = 0.0\n",
    "\n",
    "        # Evaluation results for multi-task\n",
    "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            self.trainSingleEpoch()\n",
    "            self.test()\n",
    "            print(f'Best test f1: {self.best_test_f1:.4f}')\n",
    "            print('=' * 20)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.train_losses, self.test_losses, self.train_f1, self.test_f1, self.best_train_f1, self.best_test_f1),\n",
    "            f'./save/results/single_{self.task_name}_{self.datetimestr}_{self.best_test_f1:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def trainSingleEpoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        \n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.train_losses.append(loss)\n",
    "        self.train_f1.append(f1)\n",
    "        if f1 > self.best_train_f1:\n",
    "            self.best_train_f1 = f1\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                loss += _loss.item()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        \n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_f1.append(f1)\n",
    "        if f1 > self.best_test_f1:\n",
    "            self.best_test_f1 = f1\n",
    "            self.save_model()\n",
    "\n",
    "\n",
    "    def calc_f1(self, labels, y_pred):\n",
    "        return np.array([\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='macro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='micro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='weighted')\n",
    "        ], np.float64)\n",
    "\n",
    "    def printing(self, loss, f1):\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1[0]:.4f}')\n",
    "        print(f'Micro-F1 = {f1[1]:.4f}')\n",
    "        print(f'Weighted-F1 = {f1[2]:.4f}')\n",
    "\n",
    "    def save_model(self):\n",
    "        print('Saving model...')\n",
    "        if self.task_name == 'all':\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.best_test_f1_m[0]}_seed{self.seed}.pt'\n",
    "        else:\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.best_test_f1}_seed{self.seed}.pt'\n",
    "        save(copy.deepcopy(self.model.state_dict()), filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59caaa",
   "metadata": {},
   "source": [
    "#  train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a7729",
   "metadata": {},
   "source": [
    "Below works perfectly, Just need to add remaining functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1767b4",
   "metadata": {},
   "source": [
    "Splitting train.py up and running it line by line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3129d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_PATH = './inputDir/ref/dontpatronizeme_pcl.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83276a92",
   "metadata": {},
   "source": [
    "# Bert Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b9b60",
   "metadata": {},
   "source": [
    "To fine tune a pre-trained model you need to be sure that you're using exactly the same tokenization, vocabulary, and index mapping as you used during training.\n",
    "\n",
    "The following code is an example in how to rebuilds the tokenizer that was used by the original base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e9a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be7cc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = data['country'][0]\n",
    "sentence2 = data['country'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed5495b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp import bert\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "367ebf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.bert.tokenization\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80d43b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\kevin\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'.\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 70.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 160.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 250.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 340.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1, Total size: 423.26MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "#This might work, or cell below\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    #vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy(), #The vocab file of bert for tokenizer,\n",
    "    #vocab_file=bert.tokenization.vocab_fp,\n",
    "     do_lower_case=True)\n",
    "#tokenization.FullTokenizer(vocab_file=self.vocab_fp)\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7a30e",
   "metadata": {},
   "source": [
    "The model expects its two inputs sentences to be concatenated together. This input is expected to start with a [CLS] \"This is a classification problem\" token, and each sentence should end with a [SEP] \"Separator\" token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e84e8df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 102]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec610dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efa780ed",
   "metadata": {},
   "source": [
    "Now prepend a [CLS] token, and concatenate the ragged tensors to form a single input_word_ids tensor for each example. RaggedTensor.to_tensor() zero pads to the longest sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87ed018e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.56 PiB for an array with shape (84, 104, 101, 32, 111, 110, 101, 115) and data type |S1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22012/1359034212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m sentence1 = tf.ragged.constant([\n\u001b[1;32m----> 7\u001b[1;33m     encode_sentence(s) for s in np.chararray(sentence1[:8].encode()) ])\n\u001b[0m\u001b[0;32m      8\u001b[0m sentence2 = tf.ragged.constant([\n\u001b[0;32m      9\u001b[0m     encode_sentence(s) for s in np.chararray(sentence2[:8].encode()) ])\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\defchararray.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(subtype, shape, itemsize, unicode, buffer, offset, strides, order)\u001b[0m\n\u001b[0;32m   1967\u001b[0m         \u001b[0m_globalvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1969\u001b[1;33m             self = ndarray.__new__(subtype, shape, (dtype, itemsize),\n\u001b[0m\u001b[0;32m   1970\u001b[0m                                    order=order)\n\u001b[0;32m   1971\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.56 PiB for an array with shape (84, 104, 101, 32, 111, 110, 101, 115) and data type |S1"
     ]
    }
   ],
   "source": [
    "def encode_sentence(s):\n",
    "   tokens = list(tokenizer.tokenize(s.numpy()))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "sentence1 = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in np.chararray(sentence1[:8].encode()) ])\n",
    "sentence2 = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in np.chararray(sentence2[:8].encode()) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e67b8",
   "metadata": {},
   "source": [
    "It should be noted above that the memory allocation for the training paramaters for the BERT model is exceeded by our local machines. This could however, be solved on a higher spec local machine such as  gtx 3060 or equivelent. \n",
    "\n",
    "In order to bypass the need for tokenization, a pre-alpha trained model was selected that has been used in other NLP Semeval tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc4e3811",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.56 PiB for an array with shape (84, 104, 101, 32, 111, 110, 101, 115) and data type |S1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22012/2433358793.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'[CLS]'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchararray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minput_word_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpcolormesh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_word_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\defchararray.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(subtype, shape, itemsize, unicode, buffer, offset, strides, order)\u001b[0m\n\u001b[0;32m   1967\u001b[0m         \u001b[0m_globalvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1969\u001b[1;33m             self = ndarray.__new__(subtype, shape, (dtype, itemsize),\n\u001b[0m\u001b[0;32m   1970\u001b[0m                                    order=order)\n\u001b[0;32m   1971\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.56 PiB for an array with shape (84, 104, 101, 32, 111, 110, 101, 115) and data type |S1"
     ]
    }
   ],
   "source": [
    "cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*np.chararray(sentence1[:8].encode())\n",
    "input_word_ids = tf.concat([cls, sentence1[:32], sentence2], axis=-1)\n",
    "_ = plt.pcolormesh(input_word_ids.to_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ff75a",
   "metadata": {},
   "source": [
    "ask and input type\n",
    "The model expects two additional inputs:\n",
    "\n",
    "The input mask\n",
    "The input type\n",
    "The mask allows the model to cleanly differentiate between the content and the padding. The mask has the same shape as the input_word_ids, and contains a 1 anywhere the input_word_ids is not padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee27d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(s))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(glue_dict, tokenizer):\n",
    "  num_examples = len(glue_dict[\"sentence1\"])\n",
    "\n",
    "  sentence1 = tf.ragged.constant([\n",
    "      encode_sentence(s, tokenizer)\n",
    "      for s in np.array(glue_dict[\"sentence1\"])])\n",
    "  sentence2 = tf.ragged.constant([\n",
    "      encode_sentence(s, tokenizer)\n",
    "       for s in np.array(glue_dict[\"sentence2\"])])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_s1 = tf.zeros_like(sentence1)\n",
    "  type_s2 = tf.ones_like(sentence2)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs\n",
    "\n",
    "glue_train = bert_encode(glue['train'], tokenizer)\n",
    "glue_train_labels = glue['train']['label']\n",
    "\n",
    "glue_validation = bert_encode(glue['validation'], tokenizer)\n",
    "glue_validation_labels = glue['validation']['label']\n",
    "\n",
    "glue_test = bert_encode(glue['test'], tokenizer)\n",
    "glue_test_labels  = glue['test']['label']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
