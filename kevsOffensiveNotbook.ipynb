{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b0c6a7",
   "metadata": {},
   "source": [
    "# This notebook contains a mix-mash of offenseval and Andrews current bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407d2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fire\n",
    "#pip install torch\n",
    "#pip install torchtext\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7707ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary modules from offenseval are given below. Installs may be needed as above\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from datetime import datetime\n",
    "import fire\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import html\n",
    "import re\n",
    "from transformers import (\n",
    "    AdamW, BertForSequenceClassification, BertTokenizer,\n",
    "    get_constant_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a432aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'offenseval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14616/4173334622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m from offenseval.nn import (\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mTokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'offenseval'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from datetime import datetime\n",
    "import fire\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from transformers import (\n",
    "    AdamW, BertForSequenceClassification, BertTokenizer,\n",
    "    get_constant_schedule_with_warmup\n",
    ")\n",
    "#The below functions are loaded in individually below\n",
    "#from offenseval.nn import (\n",
    "#    Tokenizer,\n",
    "#    train, evaluate, train_cycle, save_model, load_model\n",
    "#)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#I think the below mode is missing. Hopefully it doesn't cause further issues\n",
    "#model, TEXT = load_model(\"../models/bert.uncased.sample.mean06.ft.pt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea959e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for tweets based on BERT Tokenizer + NLTK's Tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_tokenizer, html_unescape=True, max_len=128):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        ----------\n",
    "        html_unescape: Boolean (default False)\n",
    "            Use or not `html.unescape` on text before tokenizing\n",
    "        \"\"\"\n",
    "        self._html_unescape = html_unescape\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self._patterns = {\n",
    "            \"<hour>\": re.compile(r\"\\d{1,2}\\:\\d{2}\"),\n",
    "            \"<year>\": re.compile(r\"(1(7|8|9)|2(0|1))\\d\\d\"),\n",
    "            \"<num>\": re.compile(r\"\\d+(\\.)?\\d*\"),\n",
    "            # @stephenhay's from https://mathiasbynens.be/demo/url-regex\n",
    "            \"<url>\": re.compile(r\"(https?|ftp)://[^\\s/$.?#].[^\\s]*\"),\n",
    "        }\n",
    "\n",
    "    def replace_patterns(self, text):\n",
    "        for repl, pattern in self._patterns.items():\n",
    "            text = pattern.sub(repl, text)\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if self._html_unescape:\n",
    "            text = html.unescape(text)\n",
    "\n",
    "        text = self.replace_patterns(text)\n",
    "        return self.bert_tokenizer.tokenize(text)[:self.max_len]\n",
    "\n",
    "    def convert_tokens_to_ids(self, *args, **kwargs):\n",
    "        return self.bert_tokenizer.convert_tokens_to_ids(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92197d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, get_target,\n",
    "          scheduler=None, max_grad_norm=None, ncols=500):\n",
    "    \"\"\"\n",
    "    Trains the model for one full epoch\n",
    "    Arguments:\n",
    "    model: torch.nn.Module\n",
    "        Model to be trained\n",
    "    iterator:\n",
    "        An iterator over the train batches\n",
    "    optimizer: torch.nn.optimizer\n",
    "        An optimizer\n",
    "    criterion:\n",
    "        Loss function\n",
    "    scheduler: (optional) A scheduler\n",
    "        Scheduler that will be called (if given) after each call to `optimizer.step()`\n",
    "    get_target: a function\n",
    "        Function receiving a batch and returning the targets\n",
    "    max_grad_norm: float (optional, default None)\n",
    "        If not none, applies gradient clipping using the given norm\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(iterator), total=len(iterator), ncols=ncols)\n",
    "    for i, batch in pbar:\n",
    "        # Zero gradients first\n",
    "        optimizer.zero_grad()\n",
    "        # We assume we always get the length\n",
    "        text, lens = batch.text\n",
    "        #target = 1. * (batch.avg > 0.6)\n",
    "        target = get_target(batch)\n",
    "\n",
    "        predictions = model(text)\n",
    "        if type(predictions) is tuple:\n",
    "            # This is because of BERTSequenceClassifier, sorry!\n",
    "            predictions = predictions[0]\n",
    "\n",
    "        loss = criterion(predictions.view(-1), target)\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        if max_grad_norm:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate metrics\n",
    "        prob_predictions = torch.sigmoid(predictions)\n",
    "        preds = torch.round(prob_predictions).detach().cpu()\n",
    "        acc = accuracy_score(preds, target.cpu())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        # Update Pbar\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        desc = f\"Loss {epoch_loss / (i+1):.3f} -- Acc {epoch_acc / (i+1):.3f} -- LR {lr*1e5:.3f}e-5\"\n",
    "        pbar.set_description(desc)\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def train_cycle(model, optimizer, criterion, scheduler,\n",
    "                train_it, dev_it, epochs, get_target, model_path,\n",
    "                monitor=\"f1\", early_stopping_tolerance=5, ncols=100):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    monitor: \"f1\" or \"loss\"\n",
    "        What to monitor for Early Stopping\n",
    "    \"\"\"\n",
    "\n",
    "    if monitor not in {\"loss\", \"f1\"}:\n",
    "        raise ValueError(\"Monitor should be 'loss' or 'f1'\")\n",
    "\n",
    "\n",
    "    pbar = tqdm(range(epochs), ncols=ncols)\n",
    "    pbar.set_description(\"Epochs\")\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "    best_report = None\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    def improves_performance(best_report, report):\n",
    "        if best_report is None:\n",
    "            return True\n",
    "\n",
    "        if monitor == \"loss\":\n",
    "            if report.loss < best_report.loss:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif monitor == \"f1\":\n",
    "            if report.macro_f1 > best_report.macro_f1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n\\nEpoch {epoch}\")\n",
    "        try:\n",
    "            train_loss, train_acc = train(\n",
    "                model, train_it, optimizer, criterion, get_target=get_target,\n",
    "                max_grad_norm=max_grad_norm, scheduler=scheduler, ncols=ncols\n",
    "            )\n",
    "            report = evaluate(\n",
    "                model, dev_it, criterion, get_target=lambda batch: batch.subtask_a\n",
    "            )\n",
    "\n",
    "            desc = f'Train: Loss: {train_loss:.3f} Acc: {train_acc*100:.2f}%'\n",
    "            desc += f'\\nVal.' + str(report)\n",
    "\n",
    "            print(desc)\n",
    "            if improves_performance(best_report, report):\n",
    "                best_report = report\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Best model so far ({report}) saved at {model_path}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= early_stopping_tolerance:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopping training!\")\n",
    "            break\n",
    "\n",
    "\n",
    "def create_criterion(device, weight_with=None):\n",
    "    \"\"\"\n",
    "    Creates a `torch.nn.BCEWithLogitsLoss`.\n",
    "    If weight_with is not None, uses class weight for the positive class\n",
    "    Arguments:\n",
    "    ----------\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    weight_with: data.Dataset\n",
    "    \"\"\"\n",
    "    if weight_with:\n",
    "        y = [row.subtask_a for row in weight_with]\n",
    "\n",
    "        class_weights = compute_class_weight('balanced', ['NOT', 'OFF'], y)\n",
    "\n",
    "        # normalize it\n",
    "        class_weights = class_weights / class_weights[0]\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([class_weights[1]]))\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300360d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, TEXT, output_path):\n",
    "    base, _ = os.path.splitext(output_path)\n",
    "    vocab_path = f\"{base}.vocab.pkl\"\n",
    "\n",
    "    torch.save(model, output_path)\n",
    "\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(TEXT, f)\n",
    "\n",
    "    print(f\"Model saved to {output_path}\")\n",
    "    print(f\"Vocab saved to {vocab_path}\")\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    base, _ = os.path.splitext(model_path)\n",
    "    vocab_path = f\"{base}.vocab.pkl\"\n",
    "\n",
    "    try:\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            TEXT = pickle.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"Returning null TEXT\")\n",
    "        TEXT = None\n",
    "\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "\n",
    "    return model, TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from  https://github.com/finiteautomata/offenseval2020/blob/master/offenseval/nn/models/bert_for_sequence.py\n",
    "from transformers import BertPreTrainedModel\n",
    "\n",
    "class BertSeqModel(nn.Module):\n",
    "    def __init__(self, bert, dropout=0.1, num_labels=1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        ---------\n",
    "        bert: BertModel\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(\n",
    "            bert.config.hidden_size,\n",
    "            num_labels\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        adapter=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adapter: a function\n",
    "            Function to be applied between BERT and the linear layer\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        if adapter:\n",
    "            pooled_output = adapter(pooled_output)\n",
    "\n",
    "        out = self.classifier(pooled_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from https://github.com/finiteautomata/offenseval2020/blob/master/offenseval/nn/models/bert_gru.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31768bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSequenceClassifier(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    BERT + GRU model\n",
    "    Inspired on Ben Trevett's implementation:\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers=1,\n",
    "                 bidirectional=False,\n",
    "                 finetune_bert=False,\n",
    "                 dropout=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "        self.finetune_bert = finetune_bert\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        if not self.finetune_bert:\n",
    "            with torch.no_grad():\n",
    "                embedded = self.bert(text)[0]\n",
    "        else:\n",
    "            embedded = self.bert(text)[0]\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        #hidden = [batch size, hid dim]\n",
    "\n",
    "        output = self.out(hidden)\n",
    "\n",
    "        #output = [batch size, out dim]\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
