{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed9d07e",
   "metadata": {},
   "source": [
    "# Semeval offense notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f67aa3",
   "metadata": {},
   "source": [
    "Code taken from https://github.com/wenliangdai/multi-task-offensive-language-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64160ef5",
   "metadata": {},
   "source": [
    "Aiming to emulate results from \n",
    "python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4131b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython in c:\\users\\kevin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.29.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da5271",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad071fef",
   "metadata": {},
   "source": [
    "Below is a list of the functions which are taken from https://github.com/wenliangdai/multi-task-offensive-language-detection .\n",
    "These files need to be ported to our system and re-adjusted for our task. Hopefully this'll work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0945448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import task_a, task_b, task_c, all_tasks, read_test_file, read_test_file_all\n",
    "from config import OLID_PATH\n",
    "from cli import get_args\n",
    "from utils import load\n",
    "from datasets import HuggingfaceDataset, HuggingfaceMTDataset, ImbalancedDatasetSampler\n",
    "from models.bert import BERT, RoBERTa\n",
    "from models.gated import GatedModel\n",
    "from models.mtl import MTL_Transformer_LSTM\n",
    "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "from trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215df2ef",
   "metadata": {},
   "source": [
    "The tasks were given by\n",
    "Sub-task A - Offensive language identification;\n",
    "Sub-task B - Automatic categorization of offense types;\n",
    "Sub-task C - Offense target identification.\n",
    "\n",
    "I think task A and B fit best with our objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178cb55d",
   "metadata": {},
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aede7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLID_PATH = '.\\inputDir\\ref'\n",
    "SAVE_PATH = './save'\n",
    "#LABEL_DICT = {\n",
    "#    'a': {'OFF': 0, 'NOT': 1},\n",
    "#    'b': {'TIN': 0, 'UNT': 1, 'NULL': 2},\n",
    "#    'c': {'IND': 0, 'GRP': 1, 'OTH': 2, 'NULL': 3}\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81af2c4",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f098559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "#import torch # Don't know what not including this could do... TODO\n",
    "import numpy as np\n",
    "\n",
    "def save(toBeSaved, filename, mode='wb'):\n",
    "    dirname = os.path.dirname(filename)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    file = open(filename, mode)\n",
    "    pickle.dump(toBeSaved, file)\n",
    "    file.close()\n",
    "\n",
    "def load(filename, mode='rb'):\n",
    "    file = open(filename, mode)\n",
    "    loaded = pickle.load(file)\n",
    "    file.close()\n",
    "    return loaded\n",
    "\n",
    "def pad_sents(sents, pad_token):\n",
    "    sents_padded = []\n",
    "    lens = get_lens(sents)\n",
    "    max_len = max(lens)\n",
    "    sents_padded = [sents[i] + [pad_token] * (max_len - l) for i, l in enumerate(lens)]\n",
    "    return sents_padded\n",
    "\n",
    "def sort_sents(sents, reverse=True):\n",
    "    sents.sort(key=(lambda s: len(s)), reverse=reverse)\n",
    "    return sents\n",
    "\n",
    "def get_mask(sents, unmask_idx=1, mask_idx=0):\n",
    "    lens = get_lens(sents)\n",
    "    max_len = max(lens)\n",
    "    mask = [([unmask_idx] * l + [mask_idx] * (max_len - l)) for l in lens]\n",
    "    return mask\n",
    "\n",
    "def get_lens(sents):\n",
    "    return [len(sent) for sent in sents]\n",
    "\n",
    "def get_max_len(sents):\n",
    "    max_len = max([len(sent) for sent in sents])\n",
    "    return max_len\n",
    "\n",
    "def truncate_sents(sents, length):\n",
    "    sents = [sent[:length] for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def get_loss_weight(labels, label_order):\n",
    "    nums = [np.sum(labels == lo) for lo in label_order]\n",
    "    loss_weight = torch.tensor([n / len(labels) for n in nums])\n",
    "    return loss_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3037a9",
   "metadata": {},
   "source": [
    "# Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46d0bd",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d814c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import wordsegment\n",
    "#from config import OLID_PATH\n",
    "#from utils import pad_sents, get_mask, get_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9272be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import wordsegment\n",
    "#from config import OLID_PATH\n",
    "#from utils import pad_sents, get_mask, get_lens\n",
    "\n",
    "wordsegment.load()\n",
    "\n",
    "def read_file(filepath: str):\n",
    "    df = pd.read_csv(filepath, sep='\\t', keep_default_na=False)\n",
    "\n",
    "    ids = np.array(df['id'].values)\n",
    "    tweets = np.array(df['tweet'].values)\n",
    "\n",
    "    # Process tweets\n",
    "    tweets = process_tweets(tweets)\n",
    "\n",
    "    label_a = np.array(df['subtask_a'].values)\n",
    "    label_b = df['subtask_b'].values\n",
    "    label_c = np.array(df['subtask_c'].values)\n",
    "    nums = len(df)\n",
    "\n",
    "    return nums, ids, tweets, label_a, label_b, label_c\n",
    "\n",
    "\n",
    "\n",
    "def read_test_file(task, tokenizer, truncate=512):\n",
    "    df1 = pd.read_csv(os.path.join(OLID_PATH, 'testset-level' + task + '.tsv'), sep='\\t')\n",
    "    df2 = pd.read_csv(os.path.join(OLID_PATH, 'labels-level' + task + '.csv'), sep=',')\n",
    "    ids = np.array(df1['id'].values)\n",
    "    tweets = np.array(df1['tweet'].values)\n",
    "    labels = np.array(df2['label'].values)\n",
    "    nums = len(df1)\n",
    "\n",
    "    # Process tweets\n",
    "    tweets = process_tweets(tweets)\n",
    "\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    lens = get_lens(token_ids)\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, labels\n",
    "\n",
    "def read_test_file_all(tokenizer, truncate=512):\n",
    "    df = pd.read_csv(os.path.join(OLID_PATH, 'testset-levela.tsv'), sep='\\t')\n",
    "    df_a = pd.read_csv(os.path.join(OLID_PATH, 'labels-levela.csv'), sep=',')\n",
    "    ids = np.array(df['id'].values)\n",
    "    tweets = np.array(df['tweet'].values)\n",
    "    label_a = np.array(df_a['label'].values)\n",
    "    nums = len(df)\n",
    "\n",
    "    # Process tweets\n",
    "    tweets = process_tweets(tweets)\n",
    "\n",
    "    df_b = pd.read_csv(os.path.join(OLID_PATH, 'labels-levelb.csv'), sep=',')\n",
    "    df_c = pd.read_csv(os.path.join(OLID_PATH, 'labels-levelc.csv'), sep=',')\n",
    "    label_data_b = dict(zip(df_b['id'].values, df_b['label'].values))\n",
    "    label_data_c = dict(zip(df_c['id'].values, df_c['label'].values))\n",
    "    label_b = [label_data_b[id] if id in label_data_b.keys() else 'NULL' for id in ids]\n",
    "    label_c = [label_data_c[id] if id in label_data_c.keys() else 'NULL' for id in ids]\n",
    "\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    lens = get_lens(token_ids)\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_a, label_b, label_c\n",
    "\n",
    "def process_tweets(tweets):\n",
    "    # Process tweets\n",
    "    tweets = emoji2word(tweets)\n",
    "    tweets = replace_rare_words(tweets)\n",
    "    tweets = remove_replicates(tweets)\n",
    "    tweets = segment_hashtag(tweets)\n",
    "    tweets = remove_useless_punctuation(tweets)\n",
    "    tweets = np.array(tweets)\n",
    "    return tweets\n",
    "\n",
    "def emoji2word(sents):\n",
    "    return [emoji.demojize(sent) for sent in sents]\n",
    "\n",
    "def remove_useless_punctuation(sents):\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent = sent.replace(':', ' ')\n",
    "        sent = sent.replace('_', ' ')\n",
    "        sent = sent.replace('...', ' ')\n",
    "        sents[i] = sent\n",
    "    return sents\n",
    "\n",
    "def remove_replicates(sents):\n",
    "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
    "    # because some tweets contain so many `@USER` which may cause redundant\n",
    "    for i, sent in enumerate(sents):\n",
    "        if sent.find('@USER') != sent.rfind('@USER'):\n",
    "            sents[i] = sent.replace('@USER', '')\n",
    "            sents[i] = '@USERS ' + sents[i]\n",
    "    return sents\n",
    "\n",
    "def replace_rare_words(sents):\n",
    "    rare_words = {\n",
    "        'URL': 'http'\n",
    "    }\n",
    "    for i, sent in enumerate(sents):\n",
    "        for w in rare_words.keys():\n",
    "            sents[i] = sent.replace(w, rare_words[w])\n",
    "    return sents\n",
    "\n",
    "def segment_hashtag(sents):\n",
    "    # E.g. '#LunaticLeft' => 'lunatic left'\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent_tokens = sent.split(' ')\n",
    "        for j, t in enumerate(sent_tokens):\n",
    "            if t.find('#') == 0:\n",
    "                sent_tokens[j] = ' '.join(wordsegment.segment(t))\n",
    "        sents[i] = ' '.join(sent_tokens)\n",
    "    return sents\n",
    "\n",
    "def all_tasks(filepath: str, tokenizer, truncate=512):\n",
    "    nums, ids, tweets, label_a, label_b, label_c = read_file(filepath)\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    lens = get_lens(token_ids)\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_a, label_b, label_c\n",
    "\n",
    "def task_1(filepath: str, tokenizer, truncate=512):\n",
    "    ids, tweets, label_1= read_file(filepath)\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    lens = get_lens(token_ids)\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_2\n",
    "\n",
    "def task_2(filepath: str, tokenizer, truncate=512):\n",
    "    ids, tweets, label_b,  = read_file(filepath)\n",
    "    # Only part of the tweets are useful for task b\n",
    "\n",
    "    useful = label_b != 'NULL'\n",
    "    ids = ids[useful]\n",
    "    tweets = tweets[useful]\n",
    "    label_b = label_b[useful]\n",
    "\n",
    "    nums = len(label_b)\n",
    "    # Tokenize\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    # Get mask\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    # Get lengths\n",
    "    lens = get_lens(token_ids)\n",
    "    # Pad tokens\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_b\n",
    "\n",
    "def task_c(filepath: str, tokenizer, truncate=512):\n",
    "    nums, ids, tweets, _, _, label_c = read_file(filepath)\n",
    "    # Only part of the tweets are useful for task c\n",
    "    useful = label_c != 'NULL'\n",
    "    ids = ids[useful]\n",
    "    tweets = tweets[useful]\n",
    "    label_c = label_c[useful]\n",
    "    nums = len(label_c)\n",
    "    # Tokenize\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    # Get mask\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    # Get lengths\n",
    "    lens = get_lens(token_ids)\n",
    "    # Pad tokens\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff96c7",
   "metadata": {},
   "source": [
    "# cli.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baab2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='BERT-Based Multi-Task Learning for Offensive Language Detection')\n",
    "\n",
    "    # Training hyper-parameters\n",
    "    parser.add_argument('-bs', '--batch-size', help='Batch size', type=int, required=True)\n",
    "    parser.add_argument('-lr', '--learning-rate', help='Learning rate', type=float, required=True)\n",
    "    parser.add_argument('-wd', '--weight-decay', help='Weight decay', type=float, required=False, default=0)\n",
    "    parser.add_argument('-ep', '--epochs', help='Number of epochs', type=int, required=True)\n",
    "    parser.add_argument('-tr', '--truncate', help='Truncate the sequence length to', type=int, required=False, default=512)\n",
    "    parser.add_argument('-pa', '--patience', help='Patience to stop training', type=int, required=False, default=5)\n",
    "    parser.add_argument('-cu', '--cuda', help='Cude device number', type=str, required=False, default='0')\n",
    "    parser.add_argument('-ta', '--task', help='Which subtask to run', type=str, required=True)\n",
    "    parser.add_argument('-mo', '--model', help='Which model to use', type=str, required=True)\n",
    "    parser.add_argument('-ms', '--model-size', help='Which size of model to use', type=str, required=False, default='base')\n",
    "    parser.add_argument('-cl', '--clip', help='Use clip to gradients', action='store_true')\n",
    "    parser.add_argument('-fr', '--freeze', help='Freeze the embedding layer or not to use less GPU memory', type=bool, required=False, default=False)\n",
    "    parser.add_argument('-lw', '--loss-weights', help='Weights for all losses', nargs='+', type=float, required=False, default=[1, 1, 1, 1])\n",
    "    parser.add_argument('-sc', '--scheduler', help='Use scheduler to optimizer', action='store_true')\n",
    "    parser.add_argument('-se', '--seed', help='Random seed', type=int, required=False, default=19951126)\n",
    "\n",
    "    parser.add_argument('--ckpt', type=str, required=False, default='')\n",
    "\n",
    "    # Transformers\n",
    "    parser.add_argument('-ad', '--attention-dropout', help='transformer attention dropout', type=float, required=False, default=0.1)\n",
    "    parser.add_argument('-hd', '--hidden-dropout', help='transformer hidden dropout', type=float, required=False, default=0.1)\n",
    "\n",
    "    # LSTM\n",
    "    parser.add_argument('-dr', '--dropout', help='dropout', type=float, required=False, default=0.1)\n",
    "    parser.add_argument('-nl', '--num-layers', help='num of layers of LSTM', type=int, required=False, default=1)\n",
    "    parser.add_argument('-hs', '--hidden-size', help='hidden vector size of LSTM', type=int, required=False, default=300)\n",
    "    parser.add_argument('-hcm', '--hidden-combine-method', help='how to combbine hidden vectors in LSTM', type=str, required=False, default='concat')\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f44d8",
   "metadata": {},
   "source": [
    "# Datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff7cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class HuggingfaceDataset(Dataset):\n",
    "    def __init__(self, input_ids, lens, mask, labels, task):\n",
    "        self.input_ids = torch.tensor(input_ids)\n",
    "        self.lens = lens\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        this_LABEL_DICT = LABEL_DICT[self.task]\n",
    "        input = self.input_ids[idx]\n",
    "        length = self.lens[idx]\n",
    "        mask = self.mask[idx]\n",
    "        label = torch.tensor(this_LABEL_DICT[self.labels[idx]])\n",
    "        return input, length, mask, label\n",
    "\n",
    "class HuggingfaceMTDataset(Dataset):\n",
    "    def __init__(self, input_ids, lens, mask, labels, task):\n",
    "        self.input_ids = torch.tensor(input_ids)\n",
    "        self.lens = lens\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels['a'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input_ids[idx]\n",
    "        mask = self.mask[idx]\n",
    "        length = self.lens[idx]\n",
    "        label_A = torch.tensor(LABEL_DICT['a'][self.labels['a'][idx]])\n",
    "        label_B = torch.tensor(LABEL_DICT['b'][self.labels['b'][idx]])\n",
    "        label_C = torch.tensor(LABEL_DICT['c'][self.labels['c'][idx]])\n",
    "        return input, length, mask, label_A, label_B, label_C\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset.labels))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset.labels[id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c5647",
   "metadata": {},
   "source": [
    "# Models.bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdc68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, model_size, args, num_labels=2):\n",
    "        super(BERT, self).__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            f'bert-{model_size}-uncased',\n",
    "            num_labels=num_labels,\n",
    "            hidden_dropout_prob=args['hidden_dropout'],\n",
    "            attention_probs_dropout_prob=args['attention_dropout']\n",
    "        )\n",
    "\n",
    "        # Freeze embeddings' parameters for saving memory\n",
    "        # for param in self.model.bert.embeddings.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs, lens, mask, labels=None):\n",
    "        outputs = self.model(inputs, attention_mask=mask)\n",
    "        logits = outputs[0]\n",
    "        # return loss, logits\n",
    "        return logits\n",
    "\n",
    "class RoBERTa(nn.Module):\n",
    "    def __init__(self, model_size, args, num_labels=2):\n",
    "        super(RoBERTa, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "            f'roberta-{model_size}',\n",
    "            num_labels=num_labels,\n",
    "            hidden_dropout_prob=args['hidden_dropout'],\n",
    "            attention_probs_dropout_prob=args['attention_dropout']\n",
    "        )\n",
    "\n",
    "        # Freeze embeddings' parameters for saving memory\n",
    "        # for param in self.model.roberta.embeddings.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs, lens, mask, labels):\n",
    "        outputs = self.model(inputs, attention_mask=mask, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        # return loss, logits\n",
    "        return logits\n",
    "\n",
    "class MTModel(nn.Module):\n",
    "    def __init__(self, model, model_size, args):\n",
    "        super(MTModel, self).__init__()\n",
    "        if model == 'bert':\n",
    "            pretrained = BertForSequenceClassification.from_pretrained(\n",
    "                f'bert-{model_size}-uncased',\n",
    "                hidden_dropout_prob=args['hidden_dropout'],\n",
    "                attention_probs_dropout_prob=args['attention_dropout']\n",
    "            )\n",
    "            self.main = pretrained.bert\n",
    "            self.dropout = pretrained.dropout\n",
    "        elif model == 'roberta':\n",
    "            pretrained = RobertaForSequenceClassification.from_pretrained(\n",
    "                f'roberta-{model_size}',\n",
    "                hidden_dropout_prob=args['hidden_dropout'],\n",
    "                attention_probs_dropout_prob=args['attention_dropout']\n",
    "            )\n",
    "            self.main = pretrained.roberta\n",
    "            self.dropout = pretrained.dropout\n",
    "\n",
    "        # Freeze embeddings' parameters for saving memory\n",
    "        for param in self.main.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        linear_in_features = 768 if model_size == 'base' else 1024\n",
    "\n",
    "        self.classifier_a = nn.Linear(in_features=linear_in_features, out_features=2, bias=True)\n",
    "        self.classifier_b = nn.Linear(in_features=linear_in_features, out_features=3, bias=True)\n",
    "        self.classifier_c = nn.Linear(in_features=linear_in_features, out_features=4, bias=True)\n",
    "\n",
    "    def forward(self, inputs, lens, mask):\n",
    "        outputs = self.main(inputs, attention_mask=mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # logits for 3 sub-tasks\n",
    "        logits_A = self.classifier_a(pooled_output)\n",
    "        logits_B = self.classifier_b(pooled_output)\n",
    "        logits_C = self.classifier_c(pooled_output)\n",
    "        return logits_A, logits_B, logits_C\n",
    "\n",
    "class BERT_LSTM(nn.Module):\n",
    "    def __init__(self, model_size, num_labels, args):\n",
    "        super(BERT_LSTM, self).__init__()\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.concat = args['hidden_combine_method'] == 'concat'\n",
    "        input_size = 768 if model_size == 'base' else 1024\n",
    "\n",
    "        self.emb = BertModel.from_pretrained(\n",
    "            f'bert-{model_size}-uncased',\n",
    "            hidden_dropout_prob=args['hidden_dropout'],\n",
    "            attention_probs_dropout_prob=args['attention_dropout']\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=args['num_layers'],\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=args['dropout'])\n",
    "        self.linear = nn.Linear(in_features=hidden_size * 2 if self.concat else hidden_size, out_features=num_labels)\n",
    "\n",
    "    def forward(self, inputs, lens, mask, labels):\n",
    "        embs = self.emb(inputs, attention_mask=mask)[0] # (batch_size, sequence_length, hidden_size)\n",
    "        _, (h_n, _) = self.lstm(input=embs) # (num_layers * num_directions, batch, hidden_size)\n",
    "        if self.concat:\n",
    "            h_n = torch.cat((h_n[0], h_n[1]), dim=1)\n",
    "        else:\n",
    "            h_n = h_n[0] + h_n[1]\n",
    "        h_n = self.dropout(h_n)\n",
    "        logits = self.linear(h_n)\n",
    "        return logits\n",
    "\n",
    "# class BERT_LSTM_MTL(nn.Module):\n",
    "#     def __init__(self, model, model_size, args, input_size=768):\n",
    "#         super(BERT_LSTM_MTL, self).__init__()\n",
    "#         hidden_size = args['hidden_size']\n",
    "\n",
    "#         self.emb = BertModel.from_pretrained(\n",
    "#             f'bert-{model_size}-uncased',\n",
    "#             hidden_dropout_prob=args['hidden_dropout'],\n",
    "#             attention_probs_dropout_prob=args['attention_dropout']\n",
    "#         )\n",
    "#         # self.main = pretrained.bert\n",
    "#         # self.dropout = pretrained.dropout\n",
    "#         self.LSTMs = nn.ModuleDict({\n",
    "#             'a': nn.LSTM(\n",
    "#                 input_size=input_size,\n",
    "#                 hidden_size=hidden_size,\n",
    "#                 num_layers=args['num_layers'],\n",
    "#                 bidirectional=True,\n",
    "#                 batch_first=True,\n",
    "#                 dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "#             ),\n",
    "#             'b': nn.LSTM(\n",
    "#                 input_size=input_size,\n",
    "#                 hidden_size=hidden_size,\n",
    "#                 num_layers=args['num_layers'],\n",
    "#                 bidirectional=True,\n",
    "#                 batch_first=True,\n",
    "#                 dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "#             ),\n",
    "#             'c': nn.LSTM(\n",
    "#                 input_size=input_size,\n",
    "#                 hidden_size=hidden_size,\n",
    "#                 num_layers=args['num_layers'],\n",
    "#                 bidirectional=True,\n",
    "#                 batch_first=True,\n",
    "#                 dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "#             )\n",
    "#         })\n",
    "#         self.dropout = nn.Dropout(p=args['dropout'])\n",
    "#         self.Linears = nn.ModuleDict({\n",
    "#             'a': nn.Linear(in_features=hidden_size * 2, out_features=2),\n",
    "#             'b': nn.Linear(in_features=hidden_size * 2, out_features=3),\n",
    "#             'c': nn.Linear(in_features=hidden_size * 2, out_features=4)\n",
    "#         })\n",
    "\n",
    "#     def forward(self, inputs, mask):\n",
    "#         embs = self.emb(inputs, attention_mask=mask)[0] # (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "#         _, (logits_a, _) = self.LSTMs['a'](embs)\n",
    "#         logits_a = torch.cat((logits_a[0], logits_a[1]), dim=1)\n",
    "#         logits_a = self.dropout(logits_a)\n",
    "#         logits_a = self.Linears['a'](logits_a)\n",
    "\n",
    "#         _, (logits_b, _) = self.LSTMs['b'](embs)\n",
    "#         logits_b = torch.cat((logits_b[0], logits_b[1]), dim=1)\n",
    "#         logits_b = self.dropout(logits_b)\n",
    "#         logits_b = self.Linears['b'](logits_b)\n",
    "\n",
    "#         _, (logits_c, _) = self.LSTMs['c'](embs)\n",
    "#         logits_c = torch.cat((logits_c[0], logits_c[1]), dim=1)\n",
    "#         logits_c = self.dropout(logits_c)\n",
    "#         logits_c = self.Linears['c'](logits_c)\n",
    "\n",
    "#         return logits_a, logits_b, logits_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c07be",
   "metadata": {},
   "source": [
    "# models.gated.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b63b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "class GatedModel(nn.Module):\n",
    "    def __init__(self, model, model_size, args):\n",
    "        super(GatedModel, self).__init__()\n",
    "        # using BERT/RoBERTa pre-trained model\n",
    "        if model == 'bert':\n",
    "            pretrainedA = BertForSequenceClassification.from_pretrained(f'bert-{model_size}-uncased')\n",
    "            self.mainA = pretrainedA.bert\n",
    "            pretrainedB = BertForSequenceClassification.from_pretrained(f'bert-{model_size}-uncased')\n",
    "            self.mainB = pretrainedB.bert\n",
    "            pretrainedC = BertForSequenceClassification.from_pretrained(f'bert-{model_size}-uncased')\n",
    "            self.mainC = pretrainedC.bert\n",
    "\n",
    "            # Freeze embeddings' parameters for saving memory\n",
    "            for p in [\n",
    "                # *self.model.robe\n",
    "                *self.mainA.embeddings.word_embeddings.parameters(),\n",
    "                *self.mainB.embeddings.word_embeddings.parameters(),\n",
    "                *self.mainC.embeddings.word_embeddings.parameters(),\n",
    "            ]:\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.dropoutA = pretrainedA.dropout\n",
    "            self.dropoutB = pretrainedB.dropout\n",
    "            self.dropoutC = pretrainedC.dropout\n",
    "            if model_size == 'base':\n",
    "                self.hidden_size = 768\n",
    "            if model_size == 'large':\n",
    "                self.hidden_size = 1024\n",
    "\n",
    "            self.linearA = nn.Linear(in_features=self.hidden_size*3, out_features=self.hidden_size, bias=True)\n",
    "            self.linearB = nn.Linear(in_features=self.hidden_size*3, out_features=self.hidden_size, bias=True)\n",
    "            self.linearC = nn.Linear(in_features=self.hidden_size*3, out_features=self.hidden_size, bias=True)\n",
    "\n",
    "            self.softmaxA = nn.Softmax(dim=1)\n",
    "            self.softmaxB = nn.Softmax(dim=1)\n",
    "            self.softmaxC = nn.Softmax(dim=1)\n",
    "\n",
    "            self.classifier_a = nn.Linear(in_features=self.hidden_size, out_features=2, bias=True)\n",
    "            self.classifier_b = nn.Linear(in_features=self.hidden_size, out_features=3, bias=True)\n",
    "            self.classifier_c = nn.Linear(in_features=self.hidden_size, out_features=4, bias=True)\n",
    "\n",
    "        elif model == 'roberta':\n",
    "            pretrainedA = RobertaForSequenceClassification.from_pretrained(f'roberta-{model_size}')\n",
    "            self.mainA = pretrainedA.roberta\n",
    "            pretrainedB = RobertaForSequenceClassification.from_pretrained(f'roberta-{model_size}')\n",
    "            self.mainB = pretrainedB.roberta\n",
    "            pretrainedC = RobertaForSequenceClassification.from_pretrained(f'roberta-{model_size}')\n",
    "            self.mainC = pretrainedC.roberta\n",
    "\n",
    "            # Freeze embeddings' parameters for saving memory\n",
    "            for p in [\n",
    "                # *self.model.robe\n",
    "                *self.mainA.embeddings.word_embeddings.parameters(),\n",
    "                *self.mainB.embeddings.word_embeddings.parameters(),\n",
    "                *self.mainC.embeddings.word_embeddings.parameters(),\n",
    "            ]:\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.dropoutA = pretrainedA.classifier\n",
    "            self.dropoutB = pretrainedB.classifier\n",
    "            self.dropoutC = pretrainedC.classifier\n",
    "\n",
    "            if model_size == 'base':\n",
    "                self.hidden_size = 768\n",
    "            if model_size == 'large':\n",
    "                self.hidden_size = 1024\n",
    "\n",
    "            self.linearA = nn.Linear(in_features=self.hidden_size*3, out_features=self.hidden_size, bias=True)\n",
    "            self.linearB = nn.Linear(in_features=self.hidden_size*3, out_features=self.hidden_size, bias=True)\n",
    "            self.linearC = nn.Linear(in_features=self.hidden_size*3, out_features=self.hidden_size, bias=True)\n",
    "\n",
    "            self.softmaxA = nn.Softmax(dim=1)\n",
    "            self.softmaxB = nn.Softmax(dim=1)\n",
    "            self.softmaxC = nn.Softmax(dim=1)\n",
    "\n",
    "            self.classifier_a = nn.Linear(in_features=self.hidden_size, out_features=2, bias=True)\n",
    "            self.classifier_b = nn.Linear(in_features=self.hidden_size, out_features=3, bias=True)\n",
    "            self.classifier_c = nn.Linear(in_features=self.hidden_size, out_features=4, bias=True)\n",
    "\n",
    "    def forward(self, inputs, lens, mask):\n",
    "        outputsA = self.mainA(inputs, attention_mask=mask)\n",
    "        pooled_outputA = outputsA[1]\n",
    "        # pooled_outputA = self.dropoutA(pooled_outputA) # batch_size * hidden_size\n",
    "\n",
    "        outputsB = self.mainB(inputs, attention_mask=mask)\n",
    "        pooled_outputB = outputsB[1]\n",
    "        # pooled_outputB = self.dropoutB(pooled_outputB) # batch_size * hidden_size\n",
    "\n",
    "        outputsC = self.mainC(inputs, attention_mask=mask)\n",
    "        pooled_outputC = outputsC[1]\n",
    "        # pooled_outputC = self.dropoutC(pooled_outputC) # batch_size * hidden_size\n",
    "\n",
    "        gateA = self.softmaxA(self.linearA(torch.cat((pooled_outputA, pooled_outputB, pooled_outputC), 1)))\n",
    "        gateB = self.softmaxB(self.linearB(torch.cat((pooled_outputA, pooled_outputB, pooled_outputC), 1)))\n",
    "        gateC = self.softmaxC(self.linearC(torch.cat((pooled_outputA, pooled_outputB, pooled_outputC), 1)))\n",
    "\n",
    "        gateA_1 = torch.ones(gateA.shape).cuda() - gateA\n",
    "        gateB_1 = torch.ones(gateB.shape).cuda() - gateB\n",
    "        gateC_1 = torch.ones(gateC.shape).cuda() - gateC\n",
    "\n",
    "        hidden_A = torch.mul(gateA_1, pooled_outputA) + torch.mul(torch.mul(gateA, 0.5), pooled_outputB) + torch.mul(torch.mul(gateA, 0.5), pooled_outputC)\n",
    "        hidden_B = torch.mul(gateB_1, pooled_outputA) + torch.mul(torch.mul(gateB, 0.5), pooled_outputB) + torch.mul(torch.mul(gateB, 0.5), pooled_outputC)\n",
    "        hidden_C = torch.mul(gateC_1, pooled_outputA) + torch.mul(torch.mul(gateC, 0.5), pooled_outputB) + torch.mul(torch.mul(gateC, 0.5), pooled_outputC)\n",
    "\n",
    "        # logits for 3 sub-tasks\n",
    "        logits_A = self.classifier_a(hidden_A)\n",
    "        logits_B = self.classifier_b(hidden_B)\n",
    "        logits_C = self.classifier_c(hidden_C)\n",
    "\n",
    "        return logits_A, logits_B, logits_C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0b2d3",
   "metadata": {},
   "source": [
    "# models.mtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, RobertaModel\n",
    "from .modules.attention import Attention\n",
    "\n",
    "class MTL_Transformer_LSTM(nn.Module):\n",
    "    def __init__(self, model, model_size, args):\n",
    "        super(MTL_Transformer_LSTM, self).__init__()\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.concat = args['hidden_combine_method'] == 'concat'\n",
    "        input_size = 768 if model_size == 'base' else 1024\n",
    "\n",
    "        if model == 'bert':\n",
    "            MODEL = BertModel\n",
    "            model_full_name = f'{model}-{model_size}-uncased'\n",
    "        elif model == 'roberta':\n",
    "            MODEL = RobertaModel\n",
    "            model_full_name = f'{model}-{model_size}'\n",
    "\n",
    "        self.emb = MODEL.from_pretrained(\n",
    "            model_full_name,\n",
    "            hidden_dropout_prob=args['hidden_dropout'],\n",
    "            attention_probs_dropout_prob=args['attention_dropout']\n",
    "        )\n",
    "\n",
    "        self.LSTMs = nn.ModuleDict({\n",
    "            'a': nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=args['num_layers'],\n",
    "                bidirectional=True,\n",
    "                batch_first=True,\n",
    "                dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "            ),\n",
    "            'b': nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=args['num_layers'],\n",
    "                bidirectional=True,\n",
    "                batch_first=True,\n",
    "                dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "            ),\n",
    "            'c': nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=args['num_layers'],\n",
    "                bidirectional=True,\n",
    "                batch_first=True,\n",
    "                dropout=args['dropout'] if args['num_layers'] > 1 else 0\n",
    "            )\n",
    "        })\n",
    "\n",
    "        self.attention_layers = nn.ModuleDict({\n",
    "            'a': Attention(hidden_size * 2),\n",
    "            'b': Attention(hidden_size * 2),\n",
    "            'c': Attention(hidden_size * 2)\n",
    "        })\n",
    "\n",
    "        self.dropout = nn.Dropout(p=args['dropout'])\n",
    "\n",
    "        linear_in_features = hidden_size * 2 if self.concat else hidden_size\n",
    "        self.Linears = nn.ModuleDict({\n",
    "            'a': nn.Sequential(\n",
    "                nn.Linear(linear_in_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, 2)\n",
    "            ),\n",
    "            'b': nn.Sequential(\n",
    "                nn.Linear(linear_in_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, 3)\n",
    "            ),\n",
    "            'c': nn.Sequential(\n",
    "                nn.Linear(linear_in_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, 4)\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def forward(self, inputs, lens, mask):\n",
    "        embs = self.emb(inputs, attention_mask=mask)[0] # (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        _, (h_a, _) = self.LSTMs['a'](embs)\n",
    "        if self.concat:\n",
    "            h_a = torch.cat((h_a[0], h_a[1]), dim=1)\n",
    "        else:\n",
    "            h_a = h_a[0] + h_a[1]\n",
    "        h_a = self.dropout(h_a)\n",
    "\n",
    "        _, (h_b, _) = self.LSTMs['b'](embs)\n",
    "        if self.concat:\n",
    "            h_b = torch.cat((h_b[0], h_b[1]), dim=1)\n",
    "        else:\n",
    "            h_b = h_b[0] + h_b[1]\n",
    "        h_b = self.dropout(h_b)\n",
    "\n",
    "        _, (h_c, _) = self.LSTMs['c'](embs)\n",
    "        if self.concat:\n",
    "            h_c = torch.cat((h_c[0], h_c[1]), dim=1)\n",
    "        else:\n",
    "            h_c = h_c[0] + h_c[1]\n",
    "        h_c = self.dropout(h_c)\n",
    "\n",
    "        logits_a = self.Linears['a'](h_a)\n",
    "        logits_b = self.Linears['b'](h_b)\n",
    "        logits_c = self.Linears['c'](h_c)\n",
    "\n",
    "        return logits_a, logits_b, logits_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df284b",
   "metadata": {},
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c7465b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import copy\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "# Local files\n",
    "#from utils import save\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class Trainer():\n",
    "    '''\n",
    "    The trainer for training models.\n",
    "    It can be used for both single and multi task training.\n",
    "    Every class function ends with _m is for multi-task training.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        epochs: int,\n",
    "        dataloaders: Dict[str, DataLoader],\n",
    "        criterion: nn.Module,\n",
    "        loss_weights: List[float],\n",
    "        clip: bool,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        device: str,\n",
    "        patience: int,\n",
    "        task_name: str,\n",
    "        model_name: str,\n",
    "        seed: int\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        self.criterion = criterion\n",
    "        self.loss_weights = loss_weights\n",
    "        self.clip = clip\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.task_name = task_name\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
    "\n",
    "        # Evaluation results\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_f1 = []\n",
    "        self.test_f1 = []\n",
    "        self.best_train_f1 = 0.0\n",
    "        self.best_test_f1 = 0.0\n",
    "\n",
    "        # Evaluation results for multi-task\n",
    "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            self.train_one_epoch()\n",
    "            self.test()\n",
    "            print(f'Best test f1: {self.best_test_f1:.4f}')\n",
    "            print('=' * 20)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.train_losses, self.test_losses, self.train_f1, self.test_f1, self.best_train_f1, self.best_test_f1),\n",
    "            f'./save/results/single_{self.task_name}_{self.datetimestr}_{self.best_test_f1:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        y_pred_all = None\n",
    "        labels_all = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if labels_all is None:\n",
    "                labels_all = labels.numpy()\n",
    "            else:\n",
    "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if y_pred_all is None:\n",
    "                    y_pred_all = y_pred\n",
    "                else:\n",
    "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(labels_all, y_pred_all, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.train_losses.append(loss)\n",
    "        self.train_f1.append(f1)\n",
    "        if f1 > self.best_train_f1:\n",
    "            self.best_train_f1 = f1\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        y_pred_all = None\n",
    "        labels_all = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if labels_all is None:\n",
    "                labels_all = labels.numpy()\n",
    "            else:\n",
    "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                loss += _loss.item()\n",
    "\n",
    "                if y_pred_all is None:\n",
    "                    y_pred_all = y_pred\n",
    "                else:\n",
    "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(labels_all, y_pred_all, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_f1.append(f1)\n",
    "        if f1 > self.best_test_f1:\n",
    "            self.best_test_f1 = f1\n",
    "            self.save_model()\n",
    "\n",
    "    def train_m(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            self.train_one_epoch_m()\n",
    "            self.test_m()\n",
    "            print(f'Best test results A: {self.best_test_f1_m[0]:.4f}')\n",
    "            print(f'Best test results B: {self.best_test_f1_m[1]:.4f}')\n",
    "            print(f'Best test results C: {self.best_test_f1_m[2]:.4f}')\n",
    "            print('=' * 20)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.train_losses, self.test_losses, self.train_f1, self.test_f1, self.best_train_f1_m, self.best_test_f1_m),\n",
    "            f'./save/results/mtl_{self.datetimestr}_{self.best_test_f1_m[0]:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def train_one_epoch_m(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "\n",
    "        y_pred_all_A = None\n",
    "        y_pred_all_B = None\n",
    "        y_pred_all_C = None\n",
    "        labels_all_A = None\n",
    "        labels_all_B = None\n",
    "        labels_all_C = None\n",
    "\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, label_A, label_B, label_C in tqdm(dataloader, desc='Training M'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            label_A = label_A.to(device=self.device)\n",
    "            label_B = label_B.to(device=self.device)\n",
    "            label_C = label_C.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                # logits_A, logits_B, logits_C = self.model(inputs, mask)\n",
    "                all_logits = self.model(inputs, lens, mask)\n",
    "                y_pred_A = all_logits[0].argmax(dim=1).cpu().numpy()\n",
    "                y_pred_B = all_logits[1][:, 0:2].argmax(dim=1)\n",
    "                y_pred_C = all_logits[2][:, 0:3].argmax(dim=1)\n",
    "\n",
    "                Non_null_index_B = label_B != LABEL_DICT['b']['NULL']\n",
    "                Non_null_label_B = label_B[Non_null_index_B]\n",
    "                Non_null_pred_B = y_pred_B[Non_null_index_B]\n",
    "\n",
    "                Non_null_index_C = label_C != LABEL_DICT['c']['NULL']\n",
    "                Non_null_label_C = label_C[Non_null_index_C]\n",
    "                Non_null_pred_C = y_pred_C[Non_null_index_C]\n",
    "\n",
    "                labels_all_A = label_A.cpu().numpy() if labels_all_A is None else np.concatenate((labels_all_A, label_A.cpu().numpy()))\n",
    "                labels_all_B = Non_null_label_B.cpu().numpy() if labels_all_B is None else np.concatenate((labels_all_B, Non_null_label_B.cpu().numpy()))\n",
    "                labels_all_C = Non_null_label_C.cpu().numpy() if labels_all_C is None else np.concatenate((labels_all_C, Non_null_label_C.cpu().numpy()))\n",
    "\n",
    "                y_pred_all_A = y_pred_A if y_pred_all_A is None else np.concatenate((y_pred_all_A, y_pred_A))\n",
    "                y_pred_all_B = Non_null_pred_B.cpu().numpy() if y_pred_all_B is None else np.concatenate((y_pred_all_B, Non_null_pred_B.cpu().numpy()))\n",
    "                y_pred_all_C = Non_null_pred_C.cpu().numpy() if y_pred_all_C is None else np.concatenate((y_pred_all_C, Non_null_pred_C.cpu().numpy()))\n",
    "\n",
    "                # f1[0] += self.calc_f1(label_A, y_pred_A)\n",
    "                # f1[1] += self.calc_f1(Non_null_label_B, Non_null_pred_B)\n",
    "                # f1[2] += self.calc_f1(Non_null_label_C, Non_null_pred_C)\n",
    "\n",
    "                _loss = self.loss_weights[0] * self.criterion(all_logits[0], label_A)\n",
    "                _loss += self.loss_weights[1] * self.criterion(all_logits[1], label_B)\n",
    "                _loss += self.loss_weights[2] * self.criterion(all_logits[2], label_C)\n",
    "                loss += _loss.item()\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1_A = f1_score(labels_all_A, y_pred_all_A, average='macro')\n",
    "        f1_B = f1_score(labels_all_B, y_pred_all_B, average='macro')\n",
    "        f1_C = f1_score(labels_all_C, y_pred_all_C, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'A: {f1_A:.4f}')\n",
    "        print(f'B: {f1_B:.4f}')\n",
    "        print(f'C: {f1_C:.4f}')\n",
    "\n",
    "        self.train_losses.append(loss)\n",
    "        self.train_f1.append([f1_A, f1_B, f1_C])\n",
    "\n",
    "        if f1_A > self.best_train_f1_m[0]:\n",
    "            self.best_train_f1_m[0] = f1_A\n",
    "        if f1_B > self.best_train_f1_m[1]:\n",
    "            self.best_train_f1_m[1] = f1_B\n",
    "        if f1_C > self.best_train_f1_m[2]:\n",
    "            self.best_train_f1_m[2] = f1_C\n",
    "\n",
    "    def test_m(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "\n",
    "        y_pred_all_A = None\n",
    "        y_pred_all_B = None\n",
    "        y_pred_all_C = None\n",
    "        labels_all_A = None\n",
    "        labels_all_B = None\n",
    "        labels_all_C = None\n",
    "\n",
    "        for inputs, lens, mask, label_A, label_B, label_C in tqdm(dataloader, desc='Test M'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            labels_all_A = label_A.numpy() if labels_all_A is None else np.concatenate((labels_all_A, label_A.numpy()))\n",
    "            labels_all_B = label_B.numpy() if labels_all_B is None else np.concatenate((labels_all_B, label_B.numpy()))\n",
    "            labels_all_C = label_C.numpy() if labels_all_C is None else np.concatenate((labels_all_C, label_C.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            label_A = label_A.to(device=self.device)\n",
    "            label_B = label_B.to(device=self.device)\n",
    "            label_C = label_C.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                all_logits = self.model(inputs, lens, mask)\n",
    "                y_pred_A = all_logits[0].argmax(dim=1).cpu().numpy()\n",
    "                y_pred_B = all_logits[1].argmax(dim=1).cpu().numpy()\n",
    "                y_pred_C = all_logits[2].argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                # f1[0] += self.calc_f1(label_A, y_pred_A)\n",
    "                # f1[1] += self.calc_f1(label_B, y_pred_B)\n",
    "                # f1[2] += self.calc_f1(label_C, y_pred_C)\n",
    "\n",
    "                y_pred_all_A = y_pred_A if y_pred_all_A is None else np.concatenate((y_pred_all_A, y_pred_A))\n",
    "                y_pred_all_B = y_pred_B if y_pred_all_B is None else np.concatenate((y_pred_all_B, y_pred_B))\n",
    "                y_pred_all_C = y_pred_C if y_pred_all_C is None else np.concatenate((y_pred_all_C, y_pred_C))\n",
    "\n",
    "                _loss = self.loss_weights[0] * self.criterion(all_logits[0], label_A)\n",
    "                _loss += self.loss_weights[1] * self.criterion(all_logits[1], label_B)\n",
    "                _loss += self.loss_weights[2] * self.criterion(all_logits[2], label_C)\n",
    "                loss += _loss.item()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1_A = f1_score(labels_all_A, y_pred_all_A, average='macro')\n",
    "        f1_B = f1_score(labels_all_B, y_pred_all_B, average='macro')\n",
    "        f1_C = f1_score(labels_all_C, y_pred_all_C, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'A: {f1_A:.4f}')\n",
    "        print(f'B: {f1_B:.4f}')\n",
    "        print(f'C: {f1_C:.4f}')\n",
    "\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_f1.append([f1_A, f1_B, f1_C])\n",
    "\n",
    "        if f1_A > self.best_test_f1_m[0]:\n",
    "            self.best_test_f1_m[0] = f1_A\n",
    "            self.save_model()\n",
    "        if f1_B > self.best_test_f1_m[1]:\n",
    "            self.best_test_f1_m[1] = f1_B\n",
    "        if f1_C > self.best_test_f1_m[2]:\n",
    "            self.best_test_f1_m[2] = f1_C\n",
    "\n",
    "        # for i in range(len(f1)):\n",
    "        #     for j in range(len(f1[0])):\n",
    "        #         if f1[i][j] > self.best_test_f1_m[i][j]:\n",
    "        #             self.best_test_f1_m[i][j] = f1[i][j]\n",
    "        #             if i == 0 and j == 0:\n",
    "        #                 self.save_model()\n",
    "\n",
    "    def calc_f1(self, labels, y_pred):\n",
    "        return np.array([\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='macro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='micro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='weighted')\n",
    "        ], np.float64)\n",
    "\n",
    "    def printing(self, loss, f1):\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1[0]:.4f}')\n",
    "        # print(f'Micro-F1 = {f1[1]:.4f}')\n",
    "        # print(f'Weighted-F1 = {f1[2]:.4f}')\n",
    "\n",
    "    def save_model(self):\n",
    "        print('Saving model...')\n",
    "        if self.task_name == 'all':\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.best_test_f1_m[0]}_seed{self.seed}.pt'\n",
    "        else:\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.best_test_f1}_seed{self.seed}.pt'\n",
    "        save(copy.deepcopy(self.model.state_dict()), filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59caaa",
   "metadata": {},
   "source": [
    "#  train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c0de2",
   "metadata": {},
   "source": [
    "Below works perfectly, Just need to add remaining functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde55cb",
   "metadata": {},
   "source": [
    "Splitting train.py up and running it line by line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2ebb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "#from data import task_a, task_b, task_c, all_tasks, read_test_file, read_test_file_all\n",
    "#from config import OLID_PATH\n",
    "#from cli import get_args\n",
    "#from utils import load\n",
    "#from datasets import HuggingfaceDataset, HuggingfaceMTDataset, ImbalancedDatasetSampler\n",
    "#from models.bert import BERT, RoBERTa\n",
    "#from models.gated import GatedModel\n",
    "#from models.mtl import MTL_Transformer_LSTM\n",
    "#from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "#from trainer import Trainer\n",
    "\n",
    "#TRAIN_PATH = os.path.join(OLID_PATH, 'olid-training-v1.0.tsv')\n",
    "TRAIN_PATH = os.path.join(OLID_PATH, 'dontpatronizeme_pcl.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "491399e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '.\\\\inputDir\\ref\\\\dontpatronizeme_pcl.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4700/465604906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mdata_methods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtask_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtask_2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_methods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mtest_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_token_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_test_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0m_Dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHuggingfaceDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4700/40901399.py\u001b[0m in \u001b[0;36mtask_1\u001b[1;34m(filepath, tokenizer, truncate)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Above will need to be redifined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtask_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4700/4166074724.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '.\\\\inputDir\\ref\\\\dontpatronizeme_pcl.tsv'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Get command line arguments (This fails in our individual model split)\n",
    "    #args = get_args()\n",
    "    #Need to pass args as follows:- \n",
    "    #python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "    # Example of defining a dict Dict = {1: 'Geeks', 2: 'For', 3: 'Geeks'}\n",
    "    \n",
    "    #Values for High Accuracy run\n",
    "    #args = {'cuda':\"1\",'seed':69,'batch_size':32,'learning_rate':3e-6,'epochs':20,'patience':3,'model':'bert','task':'a','model_size':'base','truncate':100,'weight_decay':10,'hidden_dropout':0,'attention_dropout':0,'ckpt':'','scheduler':0,'loss_weights':1,'clip':1}\n",
    "    \n",
    "    #Values for Speed run\n",
    "    args = {'cuda':\"1\",'seed':69,'batch_size':4,'learning_rate':0.3,'epochs':2,'patience':1,'model':'bert','task':1,'model_size':'base','truncate':100,'weight_decay':10,'hidden_dropout':0,'attention_dropout':0,'ckpt':'','scheduler':0,'loss_weights':1,'clip':1}\n",
    "    bs = args['batch_size']\n",
    "    lr = args['learning_rate']\n",
    "    task = args['task']\n",
    "    model_name = args['model']\n",
    "    model_size = args['model_size']\n",
    "    truncate = args['truncate']\n",
    "    epochs = args['epochs']\n",
    "    wd = args['weight_decay']\n",
    "    patience = args['patience']\n",
    "\n",
    "    # Fix seed for reproducibility\n",
    "    seed = args['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['cuda']\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_labels = 5 if task == 2 else 2\n",
    "\n",
    "    # Set tokenizer for different models\n",
    "    if model_name == 'bert':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = BERT(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = RoBERTa(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    elif model_name == 'bert-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    # Move model to correct device\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    if args['ckpt'] != '':   #This can be removed TODO\n",
    "        model.load_state_dict(load(args['ckpt']))\n",
    "\n",
    "    # Read in data depends on different subtasks\n",
    "    # label_orders = {'a': ['OFF', 'NOT'], 'b': ['TIN', 'UNT'], 'c': ['IND', 'GRP', 'OTH']}\n",
    "    if task in [1, 2]:\n",
    "        data_methods = {1: task_1, 2: task_2}\n",
    "        ids, token_ids, lens, mask, labels = data_methods[task](TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
    "        test_ids, test_token_ids, test_lens, test_mask, test_labels = read_test_file(task, tokenizer=tokenizer, truncate=truncate)\n",
    "        _Dataset = HuggingfaceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6fbd81d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '.\\\\inputDir\\ref\\\\dontpatronizeme_pcl.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4700/675457510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4700/4166074724.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '.\\\\inputDir\\ref\\\\dontpatronizeme_pcl.tsv'"
     ]
    }
   ],
   "source": [
    "read_file(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef0f3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above will need to be redifined\n",
    "def task_1(filepath: str, tokenizer, truncate=512):\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3cceb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above will need to be redifined\n",
    "def task_1(filepath: str, tokenizer, truncate=512):\n",
    "    ids, tweets, label_1= read_file(filepath)\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    lens = get_lens(token_ids)\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_2\n",
    "\n",
    "def task_2(filepath: str, tokenizer, truncate=512):\n",
    "    ids, tweets, label_b,  = read_file(filepath)\n",
    "    # Only part of the tweets are useful for task b\n",
    "\n",
    "    useful = label_b != 'NULL'\n",
    "    ids = ids[useful]\n",
    "    tweets = tweets[useful]\n",
    "    label_b = label_b[useful]\n",
    "\n",
    "    nums = len(label_b)\n",
    "    # Tokenize\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    # Get mask\n",
    "    mask = np.array(get_mask(token_ids))\n",
    "    # Get lengths\n",
    "    lens = get_lens(token_ids)\n",
    "    # Pad tokens\n",
    "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f516341",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13100/626107349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtask_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_tasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_test_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_test_file_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjit\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m  \u001b[1;31m# automatically select proper tqdm submodule if available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTqdmExperimentalWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnotebook_tqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrange\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnotebook_trange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\autonotebook.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTqdmExperimentalWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     warn(\"Using `tqdm.autonotebook.tqdm` in notebook mode.\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mIPY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# IPython 4.x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mIPY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# IPython 3.x / 2.x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipywidgets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__protocol_version__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__jupyter_widgets_controls_version__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__jupyter_widgets_base_version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwidgets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtraitlets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipywidgets\\widgets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Distributed under the terms of the Modified BSD License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwidget\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWidget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallbackDispatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregister\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidget_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdomwidget\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDOMWidget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvaluewidget\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mValueWidget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipywidgets\\widgets\\widget.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mWidget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoggingHasTraits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;31m# Class attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\traitlets\\traitlets.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(cls, name, bases, classdict)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[1;34m\"\"\"Finish initializing the HasDescriptors class.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMetaHasDescriptors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetup_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\traitlets\\traitlets.py\u001b[0m in \u001b[0;36msetup_class\u001b[1;34m(cls, classdict)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetup_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trait_default_generators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMetaHasTraits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\traitlets\\traitlets.py\u001b[0m in \u001b[0;36msetup_class\u001b[1;34m(cls, classdict)\u001b[0m\n\u001b[0;32m    816\u001b[0m                 \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetmembers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseDescriptor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m                 \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubclass_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\traitlets\\traitlets.py\u001b[0m in \u001b[0;36mgetmembers\u001b[1;34m(object, predicate)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpredicate\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    elif task in ['all']:\n",
    "        ids, token_ids, lens, mask, label_a, label_b, label_c = all_tasks(TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
    "        test_ids, test_token_ids, test_lens, test_mask, test_label_a, test_label_b, test_label_c = read_test_file_all(tokenizer)\n",
    "        labels = {'a': label_a, 'b': label_b, 'c': label_c}\n",
    "        test_labels = {'a': test_label_a, 'b': test_label_b, 'c': test_label_c}\n",
    "        _Dataset = HuggingfaceMTDataset\n",
    "\n",
    "    datasets = {\n",
    "        'train': _Dataset(\n",
    "            input_ids=token_ids,\n",
    "            lens=lens,\n",
    "            mask=mask,\n",
    "            labels=labels,\n",
    "            task=task\n",
    "        ),\n",
    "        'test': _Dataset(\n",
    "            input_ids=test_token_ids,\n",
    "            lens=test_lens,\n",
    "            mask=test_mask,\n",
    "            labels=test_labels,\n",
    "            task=task\n",
    "        )\n",
    "    }\n",
    "\n",
    "    sampler = ImbalancedDatasetSampler(datasets['train']) if task in ['a', 'b', 'c'] else None\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(\n",
    "            dataset=datasets['train'],\n",
    "            batch_size=bs,\n",
    "            sampler=sampler\n",
    "        ),\n",
    "        'test': DataLoader(dataset=datasets['test'], batch_size=bs)\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if args['scheduler']:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        # A warmup scheduler\n",
    "        t_total = epochs * len(dataloaders['train'])\n",
    "        warmup_steps = np.ceil(t_total / 10.0) * 2\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        scheduler = None\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        dataloaders=dataloaders,\n",
    "        criterion=criterion,\n",
    "        loss_weights=args['loss_weights'],\n",
    "        clip=args['clip'],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        patience=patience,\n",
    "        task_name=task,\n",
    "        model_name=model_name,\n",
    "        seed=args['seed']\n",
    "    )\n",
    "\n",
    "    if task in ['a', 'b', 'c']:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train_m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patronizing dataset should look something like this\n",
    "#python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from data import task_a, task_b, task_c, all_tasks, read_test_file, read_test_file_all\n",
    "from config import OLID_PATH\n",
    "from cli import get_args\n",
    "from utils import load\n",
    "from datasets import HuggingfaceDataset, HuggingfaceMTDataset, ImbalancedDatasetSampler\n",
    "from models.bert import BERT, RoBERTa\n",
    "from models.gated import GatedModel\n",
    "from models.mtl import MTL_Transformer_LSTM\n",
    "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "from trainer import Trainer\n",
    "\n",
    "TRAIN_PATH = os.path.join(OLID_PATH, 'olid-training-v1.0.tsv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get command line arguments (This fails in our individual model split)\n",
    "    #args = get_args()\n",
    "    #Need to pass args as follows:- \n",
    "    #python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "    # Example of defining a dict Dict = {1: 'Geeks', 2: 'For', 3: 'Geeks'}\n",
    "    \n",
    "    #Values for High Accuracy run\n",
    "    args = {'cuda':\"1\",'seed':69,'batch_size':32,'learning_rate':3e-6,'epochs':20,'patience':3,'model':'bert','task':'a','model_size':'base','truncate':100,'weight_decay':10,'hidden_dropout':0,'attention_dropout':0,'ckpt':'','scheduler':0,'loss_weights':1,'clip':1}\n",
    "    \n",
    "    #Values for Speed run\n",
    "    args = {'cuda':\"1\",'seed':69,'batch_size':8,'learning_rate':300,'epochs':10,'patience':3,'model':'bert','task':'a','model_size':'base','truncate':100,'weight_decay':10,'hidden_dropout':0,'attention_dropout':0,'ckpt':'','scheduler':0,'loss_weights':1,'clip':1}\n",
    "    bs = args['batch_size']\n",
    "    lr = args['learning_rate']\n",
    "    task = args['task']\n",
    "    model_name = args['model']\n",
    "    model_size = args['model_size']\n",
    "    truncate = args['truncate']\n",
    "    epochs = args['epochs']\n",
    "    wd = args['weight_decay']\n",
    "    patience = args['patience']\n",
    "\n",
    "    # Fix seed for reproducibility\n",
    "    seed = args['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['cuda']\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_labels = 3 if task == 'c' else 2\n",
    "\n",
    "    # Set tokenizer for different models\n",
    "    if model_name == 'bert':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = BERT(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = RoBERTa(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    elif model_name == 'bert-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "\n",
    "    # Move model to correct device\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    if args['ckpt'] != '':\n",
    "        model.load_state_dict(load(args['ckpt']))\n",
    "\n",
    "    # Read in data depends on different subtasks\n",
    "    # label_orders = {'a': ['OFF', 'NOT'], 'b': ['TIN', 'UNT'], 'c': ['IND', 'GRP', 'OTH']}\n",
    "    if task in ['a', 'b', 'c']:\n",
    "        data_methods = {'a': task_a, 'b': task_b, 'c': task_c}\n",
    "        ids, token_ids, lens, mask, labels = data_methods[task](TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
    "        test_ids, test_token_ids, test_lens, test_mask, test_labels = read_test_file(task, tokenizer=tokenizer, truncate=truncate)\n",
    "        _Dataset = HuggingfaceDataset\n",
    "    elif task in ['all']:\n",
    "        ids, token_ids, lens, mask, label_a, label_b, label_c = all_tasks(TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
    "        test_ids, test_token_ids, test_lens, test_mask, test_label_a, test_label_b, test_label_c = read_test_file_all(tokenizer)\n",
    "        labels = {'a': label_a, 'b': label_b, 'c': label_c}\n",
    "        test_labels = {'a': test_label_a, 'b': test_label_b, 'c': test_label_c}\n",
    "        _Dataset = HuggingfaceMTDataset\n",
    "\n",
    "    datasets = {\n",
    "        'train': _Dataset(\n",
    "            input_ids=token_ids,\n",
    "            lens=lens,\n",
    "            mask=mask,\n",
    "            labels=labels,\n",
    "            task=task\n",
    "        ),\n",
    "        'test': _Dataset(\n",
    "            input_ids=test_token_ids,\n",
    "            lens=test_lens,\n",
    "            mask=test_mask,\n",
    "            labels=test_labels,\n",
    "            task=task\n",
    "        )\n",
    "    }\n",
    "\n",
    "    sampler = ImbalancedDatasetSampler(datasets['train']) if task in ['a', 'b', 'c'] else None\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(\n",
    "            dataset=datasets['train'],\n",
    "            batch_size=bs,\n",
    "            sampler=sampler\n",
    "        ),\n",
    "        'test': DataLoader(dataset=datasets['test'], batch_size=bs)\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if args['scheduler']:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        # A warmup scheduler\n",
    "        t_total = epochs * len(dataloaders['train'])\n",
    "        warmup_steps = np.ceil(t_total / 10.0) * 2\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        scheduler = None\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        dataloaders=dataloaders,\n",
    "        criterion=criterion,\n",
    "        loss_weights=args['loss_weights'],\n",
    "        clip=args['clip'],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        patience=patience,\n",
    "        task_name=task,\n",
    "        model_name=model_name,\n",
    "        seed=args['seed']\n",
    "    )\n",
    "\n",
    "    if task in ['a', 'b', 'c']:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train_m()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
