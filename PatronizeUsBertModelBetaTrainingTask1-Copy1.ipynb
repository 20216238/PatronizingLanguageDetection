{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed9d07e",
   "metadata": {},
   "source": [
    "# Semeval offense notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a460d91",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be05977",
   "metadata": {},
   "source": [
    "The tasks were given by\n",
    "Sub task 1- offensive language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce04092",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ef9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '.\\inputDir\\ref'\n",
    "\n",
    "SAVE_PATH = './save'\n",
    "\n",
    "TRAIN_PATH = './inputDir/ref/dontpatronizeme_pcl.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815eaa30",
   "metadata": {},
   "source": [
    "# Function Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import wordsegment\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "import copy\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "# Local files\n",
    "#python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ac621",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a18c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(toSave, filename, mode='wb'):\n",
    "    dirname = os.path.dirname(filename)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    file = open(filename, mode)\n",
    "    pickle.dump(toSave, file)\n",
    "    file.close()\n",
    "\n",
    "def load(filename, mode='rb'):\n",
    "    file = open(filename, mode)\n",
    "    loaded = pickle.load(file)\n",
    "    file.close()\n",
    "    return loaded\n",
    "\n",
    "def tokenizationFunction(sents, pad_token):\n",
    "    sents_padded = []\n",
    "    lens = lensFinderFunction(sents)\n",
    "    max_len = max(lens)\n",
    "    sents_padded = [sents[i] + [pad_token] * (max_len - l) for i, l in enumerate(lens)]\n",
    "    return sents_padded\n",
    "\n",
    "def sortingFunction(sents, reverse=True):\n",
    "    sents.sort(key=(lambda s: len(s)), reverse=reverse)\n",
    "    return sents\n",
    "\n",
    "def maskFinderFunction(sents, unmask_idx=1, mask_idx=0):\n",
    "    lens = lensFinderFunction(sents)\n",
    "    max_len = max(lens)\n",
    "    mask = [([unmask_idx] * l + [mask_idx] * (max_len - l)) for l in lens]\n",
    "    return mask\n",
    "\n",
    "def lensFinderFunction(sents):\n",
    "    return [len(sent) for sent in sents]\n",
    "\n",
    "#def getMaskLength(sents):\n",
    "#    max_len = max([len(sent) for sent in sents])\n",
    "#    return max_len\n",
    "\n",
    "#def truncateLengthArray(sents, length):\n",
    "#    sents = [sent[:length] for sent in sents]\n",
    "#    return sents\n",
    "\n",
    "def get_loss_weight(labels, label_order):\n",
    "    nums = [np.sum(labels == lo) for lo in label_order]\n",
    "    loss_weight = torch.tensor([n / len(labels) for n in nums])\n",
    "    return loss_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dac77",
   "metadata": {},
   "source": [
    "# Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87241ec1",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb487115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm = DontPatronizeMe('.', 'dontpatronizeme_pcl.tsv')\n",
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "\n",
    "# which we can then access as a dataframe\n",
    "dpm.train_task1_df.head()\n",
    "\n",
    "data=dpm.train_task1_df\n",
    "trainData, testData = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "wordsegment.load()\n",
    "\n",
    "\n",
    "def readPatronizationFile(filepath: str):\n",
    "    ids = np.array(trainData['ids'].values)\n",
    "    text = np.array(trainData['text'].values)\n",
    "    label_1 = np.array(trainData['labels'].values)\n",
    "    \n",
    "    \n",
    "    # Process text\n",
    "    text = textProcessingFunction(text)\n",
    "    nums = len(trainData)\n",
    "    return ids,nums, text, label_1# title#, label_b, label_c\n",
    "\n",
    "\n",
    "\n",
    "def testDataCreationFunction(task, tokenizer, truncate=512):\n",
    "    ids = np.array(testData['ids'].values)\n",
    "    texts = np.array(testData['text'].values)\n",
    "    label_1 = np.array(testData['labels'].values)\n",
    "    \n",
    "    # Process text\n",
    "    texts = textProcessingFunction(texts)\n",
    "    nums = len(testData)\n",
    "    token_ids = [tokenizer.encode(text=texts[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_1\n",
    "\n",
    "def textProcessingFunction(textLines):\n",
    "    # Process textLines\n",
    "    #textLines = emoji2word(textLines)\n",
    "    #textLines = replace_rare_words(textLines)\n",
    "    textLines = remove_replicates(textLines)\n",
    "    #textLines = segment_hashtag(textLines)\n",
    "    textLines = remove_useless_punctuation(textLines)\n",
    "    textLines = np.array(textLines)\n",
    "    return textLines\n",
    "\n",
    "#def emoji2word(sents):\n",
    "#    return [emoji.demojize(sent) for sent in sents]\n",
    "\n",
    "def remove_useless_punctuation(sents):\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent = sent.replace(':', ' ')\n",
    "        sent = sent.replace('_', ' ')\n",
    "        sent = sent.replace('...', ' ')\n",
    "        sents[i] = sent\n",
    "    return sents\n",
    "\n",
    "def remove_replicates(sents):\n",
    "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
    "    # because some textLines contain so many `@USER` which may cause redundant\n",
    "    for i, sent in enumerate(sents):\n",
    "        if sent.find('@USER') != sent.rfind('@USER'):\n",
    "            sents[i] = sent.replace('@USER', '')\n",
    "            sents[i] = '@USERS ' + sents[i]\n",
    "    return sents\n",
    "\n",
    "def all_tasks(filepath: str, tokenizer, truncate=512):\n",
    "    nums, ids, textLines, label_a = readPatronizationFile(filepath)#''', label_b, label_c'''\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_a, label_b, label_c\n",
    "\n",
    "#Above will need to be redifined\n",
    "def task_1(filepath: str, tokenizer, truncate=512):\n",
    "    ids,nums, textLines, label_1= readPatronizationFile(filepath)\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_1\n",
    "\n",
    "#Below willl need to be fixed up with the other path\n",
    "def task_2(filepath: str, tokenizer, truncate=512):\n",
    "    ids, textLines, label_b,  = readPatronizationFile(filepath)\n",
    "    # Only part of the textLines are useful for task b\n",
    "\n",
    "    useful = label_b != 'NULL'\n",
    "    ids = ids[useful]\n",
    "    textLines = textLines[useful]\n",
    "    label_b = label_b[useful]\n",
    "\n",
    "    nums = len(label_b)\n",
    "    # Tokenize\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    # Get mask\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    # Get lengths\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    # Pad tokens\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac605a4",
   "metadata": {},
   "source": [
    "# Datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class PatronizationDataset(Dataset):\n",
    "    def __init__(self, input_ids, lens, mask, labels, task):\n",
    "        self.input_ids = torch.tensor(input_ids)\n",
    "        self.lens = lens\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #this_LABEL_DICT = LABEL_DICT[self.task]\n",
    "        input = self.input_ids[idx]\n",
    "        length = self.lens[idx]\n",
    "        mask = self.mask[idx]\n",
    "        #label = torch.tensor(this_LABEL_DICT[self.labels[idx]])\n",
    "        label = self.labels[idx]\n",
    "        return input, length, mask, label\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset.labels))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset.labels[id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2077ca",
   "metadata": {},
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    '''\n",
    "    The trainer for training models.\n",
    "    It can be used for both single and multi task training.\n",
    "    Every class function ends with _m is for multi-task training.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        modelNumber: nn.Module,\n",
    "        epochNumber: int,\n",
    "        dataloaderNumber: Dict[str, DataLoader],\n",
    "        criterionNumber: nn.Module,\n",
    "        loss_weightNumber: List[float],\n",
    "        clipNumber: bool,\n",
    "        optimizerNumber: torch.optim.Optimizer,\n",
    "        schedulerNumber: torch.optim.lr_scheduler,\n",
    "        deviceNumber: str,\n",
    "        patienceNumber: int,\n",
    "        taskName: str,\n",
    "        modelName: str,\n",
    "        seedNumber: int\n",
    "    ):\n",
    "        self.model = modelNumber\n",
    "        self.epochs = epochNumber\n",
    "        self.dataloaders = dataloaderNumber\n",
    "        self.criterion = criterionNumber\n",
    "        self.loss_weights = loss_weightNumber\n",
    "        self.clip = clipNumber\n",
    "        self.optimizer = optimizerNumber\n",
    "        self.scheduler = schedulerNumber\n",
    "        self.device = deviceNumber\n",
    "        self.patience = patienceNumber\n",
    "        self.task_name = taskName\n",
    "        self.model_name = modelName\n",
    "        self.seed = seedNumber\n",
    "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
    "\n",
    "        # Evaluation results\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_f1 = []\n",
    "        self.test_f1 = []\n",
    "        self.best_train_f1 = 0.0\n",
    "        self.best_test_f1 = 0.0\n",
    "\n",
    "        # Evaluation results for multi-task\n",
    "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch number {epoch}')\n",
    "            print('=' * 20)\n",
    "            print('/' * 10,'\\\\'*10)\n",
    "            self.trainSingleEpoch()\n",
    "            self.test()\n",
    "            print(f'Best test f1: {self.bestTestF1Score:.4f}')\n",
    "            print('\\\\'*10,'/' * 10)\n",
    "            print('=' * 20)\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.trainLosses, self.testingLosses, self.train_f1, self.testF1Score, self.best_train_f1, self.bestTestF1Score),\n",
    "            f'./save/results/single_{self.task_name}_{self.dateTimeString}_{self.bestTestF1Score:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def trainSingleEpoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        accuracy = accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        balancedAccuracy = balanced_accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        roc = roc_auc_score(allLabelsFound, yPredictedValue)\n",
    "\n",
    "        #TODO insert other details here\n",
    "        \n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "        print(f'Accuracy = {accuracy:.4f}')\n",
    "        print(f'Balanced Accuracy  = {balancedAccuracy:.4f}')\n",
    "        print(f'ROC value = {roc:.4f}')\n",
    "\n",
    "        self.trainLosses.append(loss)\n",
    "        self.train_f1.append(f1)\n",
    "        if f1 > self.best_train_f1:\n",
    "            self.bestTrainF1ScoreFound= f1\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                loss += _loss.item()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        accuracy = accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        balancedAccuracy = balanced_accuracy_score(allLabelsFound, yPredictedValue)\n",
    "        roc = roc_auc_score(allLabelsFound, yPredictedValue)\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "        print(f'Accuracy = {accuracy:.4f}')\n",
    "        print(f'Balanced Accuracy  = {balancedAccuracy:.4f}')\n",
    "        print(f'ROC value = {roc:.4f}')\n",
    "        #TODO Insert other details here\n",
    "        self.testingLosses.append(loss)\n",
    "        self.testF1Score.append(f1)\n",
    "        if f1 > self.bestTestF1Score:\n",
    "            self.bestTestF1Score = f1\n",
    "            self.save_model()\n",
    "\n",
    "    def train_m(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            print('/' * 10,'\\\\'*10)\n",
    "            self.trainSingleEpoch_m()\n",
    "            self.test_m()\n",
    "            print(f'Best test results A: {self.bestTestF1Score_m[0]:.4f}')\n",
    "            print(f'Best test results B: {self.bestTestF1Score_m[1]:.4f}')\n",
    "            print(f'Best test results C: {self.bestTestF1Score_m[2]:.4f}')\n",
    "            print('=' * 20)\n",
    "            print('\\\\'*10,'/' * 10)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.trainLosses, self.testingLosses, self.train_f1, self.testF1Score, self.best_train_f1_m, self.bestTestF1Score_m),\n",
    "            f'./save/results/mtl_{self.dateTimeString}_{self.bestTestF1Score_m[0]:.4f}.pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d123cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "\n",
    "#from utils import save\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class Trainer():\n",
    "    '''\n",
    "    The trainer for training models.\n",
    "    It can be used for both single and multi task training.\n",
    "    Every class function ends with _m is for multi-task training.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        epochs: int,\n",
    "        dataloaders: Dict[str, DataLoader],\n",
    "        criterion: nn.Module,\n",
    "        loss_weights: List[float],\n",
    "        clip: bool,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        device: str,\n",
    "        patience: int,\n",
    "        task_name: str,\n",
    "        model_name: str,\n",
    "        seed: int\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        self.criterion = criterion\n",
    "        self.loss_weights = loss_weights\n",
    "        self.clip = clip\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.task_name = task_name\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
    "\n",
    "        # Evaluation results\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_f1 = []\n",
    "        self.test_f1 = []\n",
    "        self.best_train_f1 = 0.0\n",
    "        self.best_test_f1 = 0.0\n",
    "\n",
    "        # Evaluation results for multi-task\n",
    "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            self.trainSingleEpoch()\n",
    "            self.test()\n",
    "            print(f'Best test f1: {self.best_test_f1:.4f}')\n",
    "            print('=' * 20)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.train_losses, self.test_losses, self.train_f1, self.test_f1, self.best_train_f1, self.best_test_f1),\n",
    "            f'./save/results/single_{self.task_name}_{self.datetimestr}_{self.best_test_f1:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def trainSingleEpoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        \n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.train_losses.append(loss)\n",
    "        self.train_f1.append(f1)\n",
    "        if f1 > self.best_train_f1:\n",
    "            self.best_train_f1 = f1\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        yPredictedValue = None\n",
    "        allLabelsFound = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if allLabelsFound is None:\n",
    "                allLabelsFound = labels.numpy()\n",
    "            else:\n",
    "                allLabelsFound = np.concatenate((allLabelsFound, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                loss += _loss.item()\n",
    "\n",
    "                if yPredictedValue is None:\n",
    "                    yPredictedValue = y_pred\n",
    "                else:\n",
    "                    yPredictedValue = np.concatenate((yPredictedValue, y_pred))\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(allLabelsFound, yPredictedValue, average='macro')\n",
    "        \n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_f1.append(f1)\n",
    "        if f1 > self.best_test_f1:\n",
    "            self.best_test_f1 = f1\n",
    "            self.save_model()\n",
    "\n",
    "\n",
    "    def calc_f1(self, labels, y_pred):\n",
    "        return np.array([\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='macro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='micro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='weighted')\n",
    "        ], np.float64)\n",
    "\n",
    "    def printing(self, loss, f1):\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1[0]:.4f}')\n",
    "        print(f'Micro-F1 = {f1[1]:.4f}')\n",
    "        print(f'Weighted-F1 = {f1[2]:.4f}')\n",
    "\n",
    "    def save_model(self):\n",
    "        print('Saving model...')\n",
    "        if self.task_name == 'all':\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.best_test_f1_m[0]}_seed{self.seed}.pt'\n",
    "        else:\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.best_test_f1}_seed{self.seed}.pt'\n",
    "        save(copy.deepcopy(self.model.state_dict()), filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59caaa",
   "metadata": {},
   "source": [
    "#  train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a7729",
   "metadata": {},
   "source": [
    "Below works perfectly, Just need to add remaining functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1767b4",
   "metadata": {},
   "source": [
    "Splitting train.py up and running it line by line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3129d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_PATH = './inputDir/ref/dontpatronizeme_pcl.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b91ce",
   "metadata": {},
   "source": [
    "# Creation of Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae64554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, model_size, args, num_labels=2):\n",
    "        super(BERT, self).__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            f'bert-{model_size}-uncased',\n",
    "            num_labels=num_labels,\n",
    "            hidden_dropout_prob=args['hidden_dropout'],\n",
    "            attention_probs_dropout_prob=args['attention_dropout']\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, lens, mask, labels=None):\n",
    "        outputs = self.model(inputs, attention_mask=mask)\n",
    "        logits = outputs[0]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533833b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "totalAccuracyList = []\n",
    "totalLossList = []\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #Values for High Accuracy run\n",
    "    #args = {'cuda':\"1\",'seed':69,'batch_size':32,'learningRate':3e-6,'epochs':20,'patience':3,'model':'bert','task':'a','model_size':'base','truncate':100,'weight_decay':0,'hidden_dropout':0,'attention_dropout':0,'ckpt':'','scheduler':0,'loss_weights':1,'clip':1}\n",
    "    args =  {'cuda':\"1\",'seed':69,'batch_size':32,'learning_rate':3e-6,'epochs':10,'patience':5,'model':'bert','task':2,'model_size':'base','truncate':70,'weight_decay':0,'hidden_dropout':0.2,'attention_dropout':0.4,'ckpt':'','scheduler':0,'loss_weights':[1, 1, 1, 1] ,'clip':1}\n",
    "    bs = args['batch_size']\n",
    "    lr = args['learningRate']\n",
    "    task = args['task']\n",
    "    model_name = args['model']\n",
    "    model_size = args['model_size']\n",
    "    truncate = args['truncate']\n",
    "    epochs = args['epochs']\n",
    "    wd = args['weight_decay']\n",
    "    patience = args['patience']\n",
    "\n",
    "\n",
    "    # Fix seed for reproducibility\n",
    "    seed = args['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['cuda']\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_labels = 5 if task == 2 else 2\n",
    "\n",
    "    # Set tokenizer for different models\n",
    "    if model_name == 'bert':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = BERT(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = RoBERTa(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    elif model_name == 'bert-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    # Move model to correct device\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    if args['ckpt'] != '':   #This can be removed TODO\n",
    "        model.load_state_dict(load(args['ckpt']))\n",
    "    if task in [1, 2]:\n",
    "        data_methods = {1: task_1, 2: task_2}\n",
    "        ids, token_ids, lens, mask, labels = data_methods[task](TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
    "        test_ids, test_token_ids, test_lens, test_mask, test_labels = testDataCreationFunction(task, tokenizer=tokenizer, truncate=truncate)\n",
    "        _Dataset = PatronizationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f516341",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    datasets = {\n",
    "        'train': _Dataset(\n",
    "            input_ids=token_ids,\n",
    "            lens=lens,\n",
    "            mask=mask,\n",
    "            labels=labels,\n",
    "            task=task\n",
    "        ),\n",
    "        'test': _Dataset(\n",
    "            input_ids=test_token_ids,\n",
    "            lens=test_lens,\n",
    "            mask=test_mask,\n",
    "            labels=test_labels,\n",
    "            task=task\n",
    "        )\n",
    "    }\n",
    "\n",
    "    sampler = ImbalancedDatasetSampler(datasets['train']) if task in [1,2] else None\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(\n",
    "            dataset=datasets['train'],\n",
    "            batch_size=bs,\n",
    "            sampler=sampler\n",
    "        ),\n",
    "        'test': DataLoader(dataset=datasets['test'], batch_size=bs)\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if args['scheduler']:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        # A warmup scheduler\n",
    "        t_total = epochs * len(dataloaders['train'])\n",
    "        warmup_steps = np.ceil(t_total / 10.0) * 2\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        scheduler = None\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        dataloaders=dataloaders,\n",
    "        criterion=criterion,\n",
    "        loss_weights=args['loss_weights'],\n",
    "        clip=args['clip'],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        patience=patience,\n",
    "        task_name=task,\n",
    "        model_name=model_name,\n",
    "        seed=args['seed']\n",
    "    )\n",
    "\n",
    "    if task in [1,2]:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train_m()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
