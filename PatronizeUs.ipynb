{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed9d07e",
   "metadata": {},
   "source": [
    "# Semeval offense notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a460d91",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be05977",
   "metadata": {},
   "source": [
    "The tasks were given by\n",
    "Sub task 1- offensive language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce04092",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ef9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '.\\inputDir\\ref'\n",
    "\n",
    "SAVE_PATH = './save'\n",
    "\n",
    "TRAIN_PATH = './inputDir/ref/dontpatronizeme_pcl.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815eaa30",
   "metadata": {},
   "source": [
    "# Function Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27de330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import wordsegment\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "import copy\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce4f2",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a18c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(toSave, filename, mode='wb'):\n",
    "    dirname = os.path.dirname(filename)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    file = open(filename, mode)\n",
    "    pickle.dump(toSave, file)\n",
    "    file.close()\n",
    "\n",
    "def load(filename, mode='rb'):\n",
    "    file = open(filename, mode)\n",
    "    loaded = pickle.load(file)\n",
    "    file.close()\n",
    "    return loaded\n",
    "\n",
    "def tokenizationFunction(sents, pad_token):\n",
    "    sents_padded = []\n",
    "    lens = lensFinderFunction(sents)\n",
    "    max_len = max(lens)\n",
    "    sents_padded = [sents[i] + [pad_token] * (max_len - l) for i, l in enumerate(lens)]\n",
    "    return sents_padded\n",
    "\n",
    "def sortingFunction(sents, reverse=True):\n",
    "    sents.sort(key=(lambda s: len(s)), reverse=reverse)\n",
    "    return sents\n",
    "\n",
    "def maskFinderFunction(sents, unmask_idx=1, mask_idx=0):\n",
    "    lens = lensFinderFunction(sents)\n",
    "    max_len = max(lens)\n",
    "    mask = [([unmask_idx] * l + [mask_idx] * (max_len - l)) for l in lens]\n",
    "    return mask\n",
    "\n",
    "def lensFinderFunction(sents):\n",
    "    return [len(sent) for sent in sents]\n",
    "\n",
    "#def getMaskLength(sents):\n",
    "#    max_len = max([len(sent) for sent in sents])\n",
    "#    return max_len\n",
    "\n",
    "#def truncateLengthArray(sents, length):\n",
    "#    sents = [sent[:length] for sent in sents]\n",
    "#    return sents\n",
    "\n",
    "def get_loss_weight(labels, label_order):\n",
    "    nums = [np.sum(labels == lo) for lo in label_order]\n",
    "    loss_weight = torch.tensor([n / len(labels) for n in nums])\n",
    "    return loss_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dac77",
   "metadata": {},
   "source": [
    "# Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87241ec1",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb487115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm = DontPatronizeMe('.', 'dontpatronizeme_pcl.tsv')\n",
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "\n",
    "# which we can then access as a dataframe\n",
    "dpm.train_task1_df.head()\n",
    "\n",
    "data=dpm.train_task1_df\n",
    "trainData, testData = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "wordsegment.load()\n",
    "\n",
    "\n",
    "def readPatronizationFile(filepath: str):\n",
    "    ids = np.array(trainData['ids'].values)\n",
    "    text = np.array(trainData['text'].values)\n",
    "    label_1 = np.array(trainData['labels'].values)\n",
    "    \n",
    "    \n",
    "    # Process text\n",
    "    text = textProcessingFunction(text)\n",
    "    nums = len(trainData)\n",
    "    return ids,nums, text, label_1# title#, label_b, label_c\n",
    "\n",
    "\n",
    "\n",
    "def testDataCreationFunction(task, tokenizer, truncate=512):\n",
    "    ids = np.array(testData['ids'].values)\n",
    "    texts = np.array(testData['text'].values)\n",
    "    label_1 = np.array(testData['labels'].values)\n",
    "    \n",
    "    # Process text\n",
    "    texts = textProcessingFunction(texts)\n",
    "    nums = len(testData)\n",
    "    token_ids = [tokenizer.encode(text=texts[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_1\n",
    "'''\n",
    "def testDataCreationFunction_all(tokenizer, truncate=512):\n",
    "    df = pd.read_csv(os.path.join(DATA_PATh, 'testset-levela.tsv'), sep='\\t')\n",
    "    df_a = pd.read_csv(os.path.join(DATA_PATh, 'labels-levela.csv'), sep=',')\n",
    "    ids = np.array(df['id'].values)\n",
    "    textLines = np.array(df['tweet'].values)\n",
    "    label_a = np.array(df_a['label'].values)\n",
    "    nums = len(df)\n",
    "\n",
    "    # Process textLines\n",
    "    textLines = textProcessingFunction(textLines)\n",
    "\n",
    "    df_b = pd.read_csv(os.path.join(DATA_PATh, 'labels-levelb.csv'), sep=',')\n",
    "    df_c = pd.read_csv(os.path.join(DATA_PATh, 'labels-levelc.csv'), sep=',')\n",
    "    label_data_b = dict(zip(df_b['id'].values, df_b['label'].values))\n",
    "    label_data_c = dict(zip(df_c['id'].values, df_c['label'].values))\n",
    "    #label_b = [label_data_b[id] if id in label_data_b.keys() else 'NULL' for id in ids]\n",
    "    #label_c = [label_data_c[id] if id in label_data_c.keys() else 'NULL' for id in ids]\n",
    "\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_a#, label_b, label_c'''\n",
    "\n",
    "def textProcessingFunction(textLines):\n",
    "    # Process textLines\n",
    "    #textLines = emoji2word(textLines)\n",
    "    #textLines = replace_rare_words(textLines)\n",
    "    textLines = remove_replicates(textLines)\n",
    "    #textLines = segment_hashtag(textLines)\n",
    "    textLines = remove_useless_punctuation(textLines)\n",
    "    textLines = np.array(textLines)\n",
    "    return textLines\n",
    "\n",
    "#def emoji2word(sents):\n",
    "#    return [emoji.demojize(sent) for sent in sents]\n",
    "\n",
    "def remove_useless_punctuation(sents):\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent = sent.replace(':', ' ')\n",
    "        sent = sent.replace('_', ' ')\n",
    "        sent = sent.replace('...', ' ')\n",
    "        sents[i] = sent\n",
    "    return sents\n",
    "\n",
    "def remove_replicates(sents):\n",
    "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
    "    # because some textLines contain so many `@USER` which may cause redundant\n",
    "    for i, sent in enumerate(sents):\n",
    "        if sent.find('@USER') != sent.rfind('@USER'):\n",
    "            sents[i] = sent.replace('@USER', '')\n",
    "            sents[i] = '@USERS ' + sents[i]\n",
    "    return sents\n",
    "'''\n",
    "def replace_rare_words(sents):\n",
    "    rare_words = {\n",
    "        'URL': 'http'\n",
    "    }\n",
    "    for i, sent in enumerate(sents):\n",
    "        for w in rare_words.keys():\n",
    "            sents[i] = sent.replace(w, rare_words[w])\n",
    "    return sents\n",
    "\n",
    "def segment_hashtag(sents):\n",
    "    # E.g. '#LunaticLeft' => 'lunatic left'\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent_tokens = sent.split(' ')\n",
    "        for j, t in enumerate(sent_tokens):\n",
    "            if t.find('#') == 0:\n",
    "                sent_tokens[j] = ' '.join(wordsegment.segment(t))\n",
    "        sents[i] = ' '.join(sent_tokens)\n",
    "    return sents\n",
    "'''\n",
    "def all_tasks(filepath: str, tokenizer, truncate=512):\n",
    "    nums, ids, textLines, label_a = readPatronizationFile(filepath)#''', label_b, label_c'''\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_a, label_b, label_c\n",
    "\n",
    "#Above will need to be redifined\n",
    "def task_1(filepath: str, tokenizer, truncate=512):\n",
    "    ids,nums, textLines, label_1= readPatronizationFile(filepath)\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_1\n",
    "\n",
    "#Below willl need to be fixed up with the other path\n",
    "def task_2(filepath: str, tokenizer, truncate=512):\n",
    "    ids, textLines, label_b,  = readPatronizationFile(filepath)\n",
    "    # Only part of the textLines are useful for task b\n",
    "\n",
    "    useful = label_b != 'NULL'\n",
    "    ids = ids[useful]\n",
    "    textLines = textLines[useful]\n",
    "    label_b = label_b[useful]\n",
    "\n",
    "    nums = len(label_b)\n",
    "    # Tokenize\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_ids = [tokenizer.encode(text=textLines[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
    "    # Get mask\n",
    "    mask = np.array(maskFinderFunction(token_ids))\n",
    "    # Get lengths\n",
    "    lens = lensFinderFunction(token_ids)\n",
    "    # Pad tokens\n",
    "    token_ids = np.array(tokenizationFunction(token_ids, tokenizer.pad_token_id))\n",
    "\n",
    "    return ids, token_ids, lens, mask, label_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac605a4",
   "metadata": {},
   "source": [
    "# Datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf21526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "#from config import LABEL_DICT\n",
    "\n",
    "class PatronizationDataset(Dataset):\n",
    "    def __init__(self, input_ids, lens, mask, labels, task):\n",
    "        self.input_ids = torch.tensor(input_ids)\n",
    "        self.lens = lens\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #this_LABEL_DICT = LABEL_DICT[self.task]\n",
    "        input = self.input_ids[idx]\n",
    "        length = self.lens[idx]\n",
    "        mask = self.mask[idx]\n",
    "        #label = torch.tensor(this_LABEL_DICT[self.labels[idx]])\n",
    "        label = self.labels[idx]\n",
    "        return input, length, mask, label\n",
    "'''\n",
    "class HuggingfaceMTDataset(Dataset):\n",
    "    def __init__(self, input_ids, lens, mask, labels, task):\n",
    "        self.input_ids = torch.tensor(input_ids)\n",
    "        self.lens = lens\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels['a'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input_ids[idx]\n",
    "        mask = self.mask[idx]\n",
    "        length = self.lens[idx]\n",
    "        label_1 = self.labels[idx]\n",
    "        #label_A = torch.tensor(LABEL_DICT['a'][self.labels['a'][idx]])\n",
    "        #label_B = torch.tensor(LABEL_DICT['b'][self.labels['b'][idx]])\n",
    "        #label_C = torch.tensor(LABEL_DICT['c'][self.labels['c'][idx]])\n",
    "        #return input, length, mask, label_A, label_B, label_C\n",
    "        return input, length, mask, label_1\n",
    "'''\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset.labels))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset.labels[id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2077ca",
   "metadata": {},
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d123cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    '''\n",
    "    The trainer for training models.\n",
    "    It can be used for both single and multi task training.\n",
    "    Every class function ends with _m is for multi-task training.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        epochs: int,\n",
    "        dataloaders: Dict[str, DataLoader],\n",
    "        criterion: nn.Module,\n",
    "        loss_weights: List[float],\n",
    "        clip: bool,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        device: str,\n",
    "        patience: int,\n",
    "        task_name: str,\n",
    "        model_name: str,\n",
    "        seed: int\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        self.criterion = criterion\n",
    "        self.loss_weights = loss_weights\n",
    "        self.clip = clip\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.task_name = task_name\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
    "\n",
    "        # Evaluation results\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_f1 = []\n",
    "        self.test_f1 = []\n",
    "        self.best_train_f1 = 0.0\n",
    "        self.best_test_f1 = 0.0\n",
    "\n",
    "        # Evaluation results for multi-task\n",
    "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch number {epoch}')\n",
    "            print('=' * 20)\n",
    "            print('/' * 10,'\\\\'*10)\n",
    "            self.train_one_epoch()\n",
    "            self.test()\n",
    "            print(f'Best test f1: {self.bestTestF1Score:.4f}')\n",
    "            print('\\\\'*10,'/' * 10)\n",
    "            print('=' * 20)\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.trainLosses, self.testingLosses, self.train_f1, self.testF1Score, self.best_train_f1, self.bestTestF1Score),\n",
    "            f'./save/results/single_{self.task_name}_{self.dateTimeString}_{self.bestTestF1Score:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        y_pred_all = None\n",
    "        labels_all = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if labels_all is None:\n",
    "                labels_all = labels.numpy()\n",
    "            else:\n",
    "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if y_pred_all is None:\n",
    "                    y_pred_all = y_pred\n",
    "                else:\n",
    "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(labels_all, y_pred_all, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.trainLosses.append(loss)\n",
    "        self.train_f1.append(f1)\n",
    "        if f1 > self.best_train_f1:\n",
    "            self.bestTrainF1ScoreFound= f1\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        y_pred_all = None\n",
    "        labels_all = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if labels_all is None:\n",
    "                labels_all = labels.numpy()\n",
    "            else:\n",
    "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                loss += _loss.item()\n",
    "\n",
    "                if y_pred_all is None:\n",
    "                    y_pred_all = y_pred\n",
    "                else:\n",
    "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1 = f1_score(labels_all, y_pred_all, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1:.4f}')\n",
    "\n",
    "        self.testingLosses.append(loss)\n",
    "        self.testF1Score.append(f1)\n",
    "        if f1 > self.bestTestF1Score:\n",
    "            self.bestTestF1Score = f1\n",
    "            self.save_model()\n",
    "\n",
    "    def train_m(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch}')\n",
    "            print('=' * 20)\n",
    "            print('/' * 10,'\\\\'*10)\n",
    "            self.train_one_epoch_m()\n",
    "            self.test_m()\n",
    "            print(f'Best test results A: {self.bestTestF1Score_m[0]:.4f}')\n",
    "            print(f'Best test results B: {self.bestTestF1Score_m[1]:.4f}')\n",
    "            print(f'Best test results C: {self.bestTestF1Score_m[2]:.4f}')\n",
    "            print('=' * 20)\n",
    "            print('\\\\'*10,'/' * 10)\n",
    "\n",
    "        print('Saving results ...')\n",
    "        save(\n",
    "            (self.trainLosses, self.testingLosses, self.train_f1, self.testF1Score, self.best_train_f1_m, self.bestTestF1Score_m),\n",
    "            f'./save/results/mtl_{self.dateTimeString}_{self.bestTestF1Score_m[0]:.4f}.pt'\n",
    "        )\n",
    "\n",
    "    def train_one_epoch_m(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "\n",
    "        y_pred_all_A = None\n",
    "        #y_pred_all_B = None\n",
    "        #y_pred_all_C = None\n",
    "        labels_all_A = None\n",
    "        #labels_all_B = None\n",
    "        #labels_all_C = None\n",
    "\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, label_A, label_B, label_C in tqdm(dataloader, desc='Training M'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            label_A = label_A.to(device=self.device)\n",
    "            label_B = label_B.to(device=self.device)\n",
    "            label_C = label_C.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                # logits_A, logits_B, logits_C = self.model(inputs, mask)\n",
    "                all_logits = self.model(inputs, lens, mask)\n",
    "                y_pred_A = all_logits[0].argmax(dim=1).cpu().numpy()\n",
    "                y_pred_B = all_logits[1][:, 0:2].argmax(dim=1)\n",
    "                y_pred_C = all_logits[2][:, 0:3].argmax(dim=1)\n",
    "\n",
    "                Non_null_index_B = label_B != LABEL_DICT['b']['NULL']\n",
    "                Non_null_label_B = label_B[Non_null_index_B]\n",
    "                Non_null_pred_B = y_pred_B[Non_null_index_B]\n",
    "\n",
    "                Non_null_index_C = label_C != LABEL_DICT['c']['NULL']\n",
    "                Non_null_label_C = label_C[Non_null_index_C]\n",
    "                Non_null_pred_C = y_pred_C[Non_null_index_C]\n",
    "\n",
    "                labels_all_A = label_A.cpu().numpy() if labels_all_A is None else np.concatenate((labels_all_A, label_A.cpu().numpy()))\n",
    "                labels_all_B = Non_null_label_B.cpu().numpy() if labels_all_B is None else np.concatenate((labels_all_B, Non_null_label_B.cpu().numpy()))\n",
    "                labels_all_C = Non_null_label_C.cpu().numpy() if labels_all_C is None else np.concatenate((labels_all_C, Non_null_label_C.cpu().numpy()))\n",
    "\n",
    "                y_pred_all_A = y_pred_A if y_pred_all_A is None else np.concatenate((y_pred_all_A, y_pred_A))\n",
    "                y_pred_all_B = Non_null_pred_B.cpu().numpy() if y_pred_all_B is None else np.concatenate((y_pred_all_B, Non_null_pred_B.cpu().numpy()))\n",
    "                y_pred_all_C = Non_null_pred_C.cpu().numpy() if y_pred_all_C is None else np.concatenate((y_pred_all_C, Non_null_pred_C.cpu().numpy()))\n",
    "\n",
    "                # f1[0] += self.calc_f1(label_A, y_pred_A)\n",
    "                # f1[1] += self.calc_f1(Non_null_label_B, Non_null_pred_B)\n",
    "                # f1[2] += self.calc_f1(Non_null_label_C, Non_null_pred_C)\n",
    "\n",
    "                _loss = self.loss_weights[0] * self.criterion(all_logits[0], label_A)\n",
    "                _loss += self.loss_weights[1] * self.criterion(all_logits[1], label_B)\n",
    "                _loss += self.loss_weights[2] * self.criterion(all_logits[2], label_C)\n",
    "                loss += _loss.item()\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1_A = f1_score(labels_all_A, y_pred_all_A, average='macro')\n",
    "        f1_B = f1_score(labels_all_B, y_pred_all_B, average='macro')\n",
    "        f1_C = f1_score(labels_all_C, y_pred_all_C, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'A: {f1_A:.4f}')\n",
    "        print(f'B: {f1_B:.4f}')\n",
    "        print(f'C: {f1_C:.4f}')\n",
    "\n",
    "        self.trainLosses.append(loss)\n",
    "        self.train_f1.append([f1_A, f1_B, f1_C])\n",
    "\n",
    "        if f1_A > self.best_train_f1_m[0]:\n",
    "            self.best_train_f1_m[0] = f1_A\n",
    "        if f1_B > self.best_train_f1_m[1]:\n",
    "            self.best_train_f1_m[1] = f1_B\n",
    "        if f1_C > self.best_train_f1_m[2]:\n",
    "            self.best_train_f1_m[2] = f1_C\n",
    "\n",
    "    def test_m(self):\n",
    "        self.model.eval()\n",
    "        dataloader = self.dataloaders['test']\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "\n",
    "        y_pred_all_A = None\n",
    "        y_pred_all_B = None\n",
    "        y_pred_all_C = None\n",
    "        labels_all_A = None\n",
    "        labels_all_B = None\n",
    "        labels_all_C = None\n",
    "\n",
    "        for inputs, lens, mask, label_A, label_B, label_C in tqdm(dataloader, desc='Test M'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            labels_all_A = label_A.numpy() if labels_all_A is None else np.concatenate((labels_all_A, label_A.numpy()))\n",
    "            labels_all_B = label_B.numpy() if labels_all_B is None else np.concatenate((labels_all_B, label_B.numpy()))\n",
    "            labels_all_C = label_C.numpy() if labels_all_C is None else np.concatenate((labels_all_C, label_C.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            label_A = label_A.to(device=self.device)\n",
    "            label_B = label_B.to(device=self.device)\n",
    "            label_C = label_C.to(device=self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                all_logits = self.model(inputs, lens, mask)\n",
    "                y_pred_A = all_logits[0].argmax(dim=1).cpu().numpy()\n",
    "                y_pred_B = all_logits[1].argmax(dim=1).cpu().numpy()\n",
    "                y_pred_C = all_logits[2].argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                # f1[0] += self.calc_f1(label_A, y_pred_A)\n",
    "                # f1[1] += self.calc_f1(label_B, y_pred_B)\n",
    "                # f1[2] += self.calc_f1(label_C, y_pred_C)\n",
    "\n",
    "                y_pred_all_A = y_pred_A if y_pred_all_A is None else np.concatenate((y_pred_all_A, y_pred_A))\n",
    "                y_pred_all_B = y_pred_B if y_pred_all_B is None else np.concatenate((y_pred_all_B, y_pred_B))\n",
    "                y_pred_all_C = y_pred_C if y_pred_all_C is None else np.concatenate((y_pred_all_C, y_pred_C))\n",
    "\n",
    "                _loss = self.loss_weights[0] * self.criterion(all_logits[0], label_A)\n",
    "                _loss += self.loss_weights[1] * self.criterion(all_logits[1], label_B)\n",
    "                _loss += self.loss_weights[2] * self.criterion(all_logits[2], label_C)\n",
    "                loss += _loss.item()\n",
    "\n",
    "        loss /= iters_per_epoch\n",
    "        f1_A = f1_score(labels_all_A, y_pred_all_A, average='macro')\n",
    "        f1_B = f1_score(labels_all_B, y_pred_all_B, average='macro')\n",
    "        f1_C = f1_score(labels_all_C, y_pred_all_C, average='macro')\n",
    "\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'A: {f1_A:.4f}')\n",
    "        print(f'B: {f1_B:.4f}')\n",
    "        print(f'C: {f1_C:.4f}')\n",
    "\n",
    "        self.testingLosses.append(loss)\n",
    "        self.testF1Score.append([f1_A, f1_B, f1_C])\n",
    "\n",
    "        if f1_A > self.bestTestF1Score_m[0]:\n",
    "            self.bestTestF1Score_m[0] = f1_A\n",
    "            self.save_model()\n",
    "        if f1_B > self.bestTestF1Score_m[1]:\n",
    "            self.bestTestF1Score_m[1] = f1_B\n",
    "        if f1_C > self.bestTestF1Score_m[2]:\n",
    "            self.bestTestF1Score_m[2] = f1_C\n",
    "    def calc_f1(self, labels, y_pred):\n",
    "        return np.array([\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='macro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='micro'),\n",
    "            f1_score(labels.cpu(), y_pred.cpu(), average='weighted')\n",
    "        ], np.float64)\n",
    "\n",
    "    def printing(self, loss, f1):\n",
    "        print(f'loss = {loss:.4f}')\n",
    "        print(f'Macro-F1 = {f1[0]:.4f}')\n",
    "        # print(f'Micro-F1 = {f1[1]:.4f}')\n",
    "        # print(f'Weighted-F1 = {f1[2]:.4f}')\n",
    "\n",
    "    def save_model(self):\n",
    "        print('Saving model...')\n",
    "        if self.task_name == 'all':\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.bestTestF1Score_m[0]}_seed{self.seed}.pt'\n",
    "        else:\n",
    "            filename = f'./save/models/{self.task_name}_{self.model_name}_{self.bestTestF1Score}_seed{self.seed}.pt'\n",
    "        save(copy.deepcopy(self.model.state_dict()), filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a37087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        y_pred_all = None\n",
    "        labels_all = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if labels_all is None:\n",
    "                labels_all = labels.numpy()\n",
    "            else:\n",
    "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1bf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        dataloader = self.dataloaders['train']\n",
    "        y_pred_all = None\n",
    "        labels_all = None\n",
    "        loss = 0\n",
    "        iters_per_epoch = 0\n",
    "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
    "            iters_per_epoch += 1\n",
    "\n",
    "            if labels_all is None:\n",
    "                labels_all = labels.numpy()\n",
    "            else:\n",
    "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
    "\n",
    "            inputs = inputs.to(device=self.device)\n",
    "            lens = lens.to(device=self.device)\n",
    "            mask = mask.to(device=self.device)\n",
    "            labels = labels.to(device=self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                logits = self.model(inputs, lens, mask, labels)\n",
    "                _loss = self.criterion(logits, labels)\n",
    "                loss += _loss.item()\n",
    "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "                if y_pred_all is None:\n",
    "                    y_pred_all = y_pred\n",
    "                else:\n",
    "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
    "\n",
    "                # Backward\n",
    "                _loss.backward()\n",
    "                if self.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59caaa",
   "metadata": {},
   "source": [
    "#  train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a7729",
   "metadata": {},
   "source": [
    "Below works perfectly, Just need to add remaining functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1767b4",
   "metadata": {},
   "source": [
    "Splitting train.py up and running it line by line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f3129d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python train.py -bs=32 -lr=3e-6 -ep=20 -pa=3 --model=bert --task=a --clip --cuda=1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_PATH = './inputDir/ref/dontpatronizeme_pcl.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533833b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Values for High Accuracy run\n",
    "    #args = {'cuda':\"1\",'seed':69,'batch_size':32,'learning_rate':3e-6,'epochs':20,'patience':3,'model':'bert','task':'a','model_size':'base','truncate':100,'weight_decay':10,'hidden_dropout':0,'attention_dropout':0,'ckpt':'','scheduler':0,'loss_weights':1,'clip':1}\n",
    "    args = {'cuda':\"1\",'seed':69,'batch_size':32,'learning_rate':3e-6,'epochs':20,'patience':5,'model':'bert','task':1,'model_size':'base','truncate':50,'weight_decay':0,'hidden_dropout':0.2,'attention_dropout':0.5,'ckpt':'','scheduler':0,'loss_weights':[1, 1, 1, 1] ,'clip':1}\n",
    "    bs = args['batch_size']\n",
    "    lr = args['learning_rate']\n",
    "    task = args['task']\n",
    "    model_name = args['model']\n",
    "    model_size = args['model_size']\n",
    "    truncate = args['truncate']\n",
    "    epochs = args['epochs']\n",
    "    wd = args['weight_decay']\n",
    "    patience = args['patience']\n",
    "\n",
    "    # Fix seed for reproducibility\n",
    "    seed = args['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['cuda']\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_labels = 5 if task == 2 else 2\n",
    "\n",
    "    # Set tokenizer for different models\n",
    "    if model_name == 'bert':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = BERT(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta':\n",
    "        if task == 'all':\n",
    "            model = MTL_Transformer_LSTM(model_name, model_size, args=args)\n",
    "        else:\n",
    "            model = RoBERTa(model_size, args=args, num_labels=num_labels)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    elif model_name == 'bert-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
    "    elif model_name == 'roberta-gate' and task == 'all':\n",
    "        model_name = model_name.replace('-gate', '')\n",
    "        model = GatedModel(model_name, model_size, args=args)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
    "    # Move model to correct device\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    if args['ckpt'] != '':   #This can be removed TODO\n",
    "        model.load_state_dict(load(args['ckpt']))\n",
    "    if task in [1, 2]:\n",
    "        data_methods = {1: task_1, 2: task_2}\n",
    "        ids, token_ids, lens, mask, labels = data_methods[task](TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
    "        test_ids, test_token_ids, test_lens, test_mask, test_labels = testDataCreationFunction(task, tokenizer=tokenizer, truncate=truncate)\n",
    "        _Dataset = PatronizationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f516341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "====================\n",
      "////////// \\\\\\\\\\\\\\\\\\\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Œ                                                                     | 2/266 [01:08<2:31:05, 34.34s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    datasets = {\n",
    "        'train': _Dataset(\n",
    "            input_ids=token_ids,\n",
    "            lens=lens,\n",
    "            mask=mask,\n",
    "            labels=labels,\n",
    "            task=task\n",
    "        ),\n",
    "        'test': _Dataset(\n",
    "            input_ids=test_token_ids,\n",
    "            lens=test_lens,\n",
    "            mask=test_mask,\n",
    "            labels=test_labels,\n",
    "            task=task\n",
    "        )\n",
    "    }\n",
    "\n",
    "    sampler = ImbalancedDatasetSampler(datasets['train']) if task in [1,2] else None\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(\n",
    "            dataset=datasets['train'],\n",
    "            batch_size=bs,\n",
    "            sampler=sampler\n",
    "        ),\n",
    "        'test': DataLoader(dataset=datasets['test'], batch_size=bs)\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if args['scheduler']:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        # A warmup scheduler\n",
    "        t_total = epochs * len(dataloaders['train'])\n",
    "        warmup_steps = np.ceil(t_total / 10.0) * 2\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        scheduler = None\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        dataloaders=dataloaders,\n",
    "        criterion=criterion,\n",
    "        loss_weights=args['loss_weights'],\n",
    "        clip=args['clip'],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        patience=patience,\n",
    "        task_name=task,\n",
    "        model_name=model_name,\n",
    "        seed=args['seed']\n",
    "    )\n",
    "\n",
    "    if task in [1,2]:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train_m()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
