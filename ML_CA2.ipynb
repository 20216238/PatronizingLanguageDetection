{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8wMyt8ETwyAs"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22068/4073796740.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kevin\\AppData\\Local\\Temp/ipykernel_22068/4073796740.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install tensorflow-text\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers\n",
    "#pip install tensorflow-text\n",
    "#pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s6AwLZ8CvgXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
      "Collecting py-params>=0.9.6\n",
      "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm->params-flow>=0.8.0->bert-for-tf2) (0.4.4)\n",
      "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
      "  Building wheel for bert-for-tf2 (setup.py): started\n",
      "  Building wheel for bert-for-tf2 (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=07b0744d4e53bf1a559c7519ade1322571b0c17182c1daac2e30bc23cb3e1102\n",
      "  Stored in directory: c:\\users\\kevin\\appdata\\local\\pip\\cache\\wheels\\ab\\a4\\72\\df07592cea3ae06b5e846f5e52262f8b16748e829ca354b7df\n",
      "  Building wheel for params-flow (setup.py): started\n",
      "  Building wheel for params-flow (setup.py): finished with status 'done'\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=e19e62073e8929d9f5b47cc26893bcc9fd5d787e7a059e83c5323e8ba20c2980\n",
      "  Stored in directory: c:\\users\\kevin\\appdata\\local\\pip\\cache\\wheels\\c7\\f3\\85\\b8cf1d8bfe55dc2ece0f1fcd4e91d6f8fc7b59ff3fd75329e1\n",
      "  Building wheel for py-params (setup.py): started\n",
      "  Building wheel for py-params (setup.py): finished with status 'done'\n",
      "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7912 sha256=f07bbe0a5bdcbba8013129120b1aee74903b5d840ed34f8562addc6da3265799\n",
      "  Stored in directory: c:\\users\\kevin\\appdata\\local\\pip\\cache\\wheels\\ac\\26\\e9\\df16869ccbd4abf517f1ff3be9a2c7ee5c5980fc87eea04fb1\n",
      "Successfully built bert-for-tf2 params-flow py-params\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZGk8xTMbUXIZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import re\n",
    "#from official.nlp import \n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "import math\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6VEhc3IKWLWC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lI1yeVaY6SLb"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrb9dwBAWDNK",
    "outputId": "609b973d-f311-42f9-ad43-2a1ed9bc7a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
     ]
    }
   ],
   "source": [
    "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "utJkruAfWFn9"
   },
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "# Initialize a dpm (Don't Patronize Me) object.\n",
    "# It takes two areguments as input: \n",
    "# (1) Path to the training set, which is the root directory of this notebook.\n",
    "# (2) Path to the test set, which will be released when the evaluation phase begins. \n",
    "# For now, you can just use the dataset for Subtask 1, which the code will load without labels.\n",
    "dpm = DontPatronizeMe('.', 'dontpatronizeme_pcl.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "06E4mdPbWFca",
    "outputId": "5169c07f-6fe3-4da5-bdf3-7df51bde6996"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@23953477</td>\n",
       "      <td>in-need</td>\n",
       "      <td>in</td>\n",
       "      <td>The ones in need of constant medical care are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@4703096</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>jm</td>\n",
       "      <td>NBC and Spanish-language Univision both declin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@25567226</td>\n",
       "      <td>in-need</td>\n",
       "      <td>hk</td>\n",
       "      <td>A second T-Home project is being launched in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@1824078</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>tz</td>\n",
       "      <td>Camfed would like to see this trend reversed ....</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@1921089</td>\n",
       "      <td>refugee</td>\n",
       "      <td>tz</td>\n",
       "      <td>Kagunga village was reported to lack necessary...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       par_id         art_id keyword  ... text label  orig_label\n",
       "0  @@23953477        in-need      in  ...    0     0           0\n",
       "1   @@4703096      immigrant      jm  ...    0     0           0\n",
       "2  @@25567226        in-need      hk  ...    0     0           0\n",
       "3   @@1824078  poor-families      tz  ...    4     1           4\n",
       "4   @@1921089        refugee      tz  ...    0     0           0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "# which we can then access as a dataframe\n",
    "dpm.train_task1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAozirumQek7"
   },
   "source": [
    "getting some grief with the file upload. Also need to figure out what type of l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BvgRI2c5n6wV"
   },
   "outputs": [],
   "source": [
    "data=dpm.train_task1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8-7VDs73i96t"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "#from tensorflow.keras.losses import softmax_cross_entropy_with_logits\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jFFkbXd5i_At"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_kt3bcVGj1-"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcYoOaWFnYDt",
    "outputId": "206a2e13-bf09-439d-d02f-d33a7ffc87d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "6N96jvFNv8Ed"
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vsgRVBBlVBzt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3dARVciz7it"
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3uqGI1DGVB5S",
    "outputId": "81fb29d3-2455-4c29-bf02-f57e8f32efa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@23953477</td>\n",
       "      <td>in-need</td>\n",
       "      <td>in</td>\n",
       "      <td>The ones in need of constant medical care are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@4703096</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>jm</td>\n",
       "      <td>NBC and Spanish-language Univision both declin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@25567226</td>\n",
       "      <td>in-need</td>\n",
       "      <td>hk</td>\n",
       "      <td>A second T-Home project is being launched in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@1824078</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>tz</td>\n",
       "      <td>Camfed would like to see this trend reversed ....</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@1921089</td>\n",
       "      <td>refugee</td>\n",
       "      <td>tz</td>\n",
       "      <td>Kagunga village was reported to lack necessary...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       par_id         art_id keyword  ... text label  orig_label\n",
       "0  @@23953477        in-need      in  ...    0     0           0\n",
       "1   @@4703096      immigrant      jm  ...    0     0           0\n",
       "2  @@25567226        in-need      hk  ...    0     0           0\n",
       "3   @@1824078  poor-families      tz  ...    4     1           4\n",
       "4   @@1921089        refugee      tz  ...    0     0           0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6M-5cSoYB6i",
    "outputId": "6b8bd506-6e8b-41be-8520-6bd4ee290ed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10636"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5CrYS9oXrCK8"
   },
   "outputs": [],
   "source": [
    "df = data.drop('par_id', 1)\n",
    "df = df.drop('art_id',1)\n",
    "df = df.drop('keyword',1)\n",
    "df = df.drop('text',1)\n",
    "#df = data.drop('article_id', 1)\n",
    "df = df.rename(columns={'country': 'paragraph'})\n",
    "#again this needs to be fixed but atm it removed the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "kEEoGM4oom5M"
   },
   "outputs": [],
   "source": [
    "#df = df.drop('paragraph.1',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pvXy21Mmr_fv",
    "outputId": "c811d2ea-c22f-4d36-b0c4-600eba8da4ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ones in need of constant medical care are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBC and Spanish-language Univision both declin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A second T-Home project is being launched in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camfed would like to see this trend reversed ....</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kagunga village was reported to lack necessary...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph  label orig_label\n",
       "0  The ones in need of constant medical care are ...      0          0\n",
       "1  NBC and Spanish-language Univision both declin...      0          0\n",
       "2  A second T-Home project is being launched in t...      0          0\n",
       "3  Camfed would like to see this trend reversed ....      1          4\n",
       "4  Kagunga village was reported to lack necessary...      0          0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qsXbC_glvmbm"
   },
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rGQX6OEuIO-",
    "outputId": "20449bc9-5778-4e96-f1eb-ade353b70178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10636"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-6uCerAsdQ6"
   },
   "source": [
    "# attempt 29th nov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnI8o4cRsgIz"
   },
   "source": [
    "so following tutorial on this one from here - starting with the data set with only the labels and the paragraphs. completely copied so far, no variables changed etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QvWer6dptcud",
    "outputId": "8a5bda5e-0416-4e21-ffc9-2715ea584363"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@23953477</td>\n",
       "      <td>in-need</td>\n",
       "      <td>in</td>\n",
       "      <td>The ones in need of constant medical care are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@4703096</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>jm</td>\n",
       "      <td>NBC and Spanish-language Univision both declin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@25567226</td>\n",
       "      <td>in-need</td>\n",
       "      <td>hk</td>\n",
       "      <td>A second T-Home project is being launched in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@1824078</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>tz</td>\n",
       "      <td>Camfed would like to see this trend reversed ....</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@1921089</td>\n",
       "      <td>refugee</td>\n",
       "      <td>tz</td>\n",
       "      <td>Kagunga village was reported to lack necessary...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       par_id         art_id keyword  ... text label  orig_label\n",
       "0  @@23953477        in-need      in  ...    0     0           0\n",
       "1   @@4703096      immigrant      jm  ...    0     0           0\n",
       "2  @@25567226        in-need      hk  ...    0     0           0\n",
       "3   @@1824078  poor-families      tz  ...    4     1           4\n",
       "4   @@1921089        refugee      tz  ...    0     0           0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WXpEF5WwtelT"
   },
   "outputs": [],
   "source": [
    "df2=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FYCXld4fspQ7"
   },
   "outputs": [],
   "source": [
    "#df2=df2.drop(\"country\",1)\n",
    "df2=df2.drop(\"art_id\",1)\n",
    "df2=df2.drop(\"par_id\",1)\n",
    "df2=df2.drop(\"keyword\",1)\n",
    "df2=df2.drop(\"text\",1)\n",
    "df2=df2.drop(\"orig_label\",1)\n",
    "df2 = df2.rename(columns={'country': 'paragraph'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QnyjxqhJuBQr",
    "outputId": "a2379cdc-3b05-4117-fba3-75fc6a00d4e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ones in need of constant medical care are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBC and Spanish-language Univision both declin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A second T-Home project is being launched in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camfed would like to see this trend reversed ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kagunga village was reported to lack necessary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph  label\n",
       "0  The ones in need of constant medical care are ...      0\n",
       "1  NBC and Spanish-language Univision both declin...      0\n",
       "2  A second T-Home project is being launched in t...      0\n",
       "3  Camfed would like to see this trend reversed ....      1\n",
       "4  Kagunga village was reported to lack necessary...      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CLvgW5bquMn6"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UToUc9_ls2B8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Ed5GkO7Ms178"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kzdhhCUktBLj"
   },
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GOUWYMl7s1yr"
   },
   "outputs": [],
   "source": [
    "pars = []\n",
    "sentences = list(df2['paragraph'])\n",
    "for sen in sentences:\n",
    "    pars.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rHjJHlaRtLnN"
   },
   "outputs": [],
   "source": [
    "y=df2[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r64PPV9ru03T"
   },
   "source": [
    "# Creating a BERT Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Ng3YtawHtLem"
   },
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kz5Jk09_u3OL"
   },
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bE72dP_rwzMa"
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = [tokenize_reviews(review) for review in pars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFEb-vfq2UyI"
   },
   "source": [
    "### preparing data for training/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLEjvqPS2eIo"
   },
   "source": [
    " To train the model, the input sentences should be of equal length. To create sentences of equal length, one way is to pad the shorter sentences by 0s. However, this can result in a sparse matrix contain large number of 0s. The other way is to pad sentences within each batch. Since we will be training the model in batches, we can pad the sentences within the training batch locally depending upon the length of the longest sentence. To do so, we first need to find the length of each sentence.\n",
    "\n",
    "The following script creates a list of lists where each sublist contains tokenized review, the label of the review and the length of the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "XsgP9tGSx0Si"
   },
   "outputs": [],
   "source": [
    "reviews_with_len = [[review, y[i], len(review)]\n",
    "                 for i, review in enumerate(tokenized_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zCjQitleyuhy"
   },
   "outputs": [],
   "source": [
    "random.shuffle(reviews_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BmZH6M-Ju3J0"
   },
   "outputs": [],
   "source": [
    "reviews_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLOOG_Xu209_"
   },
   "source": [
    "Once the reviews are sorted by length, we can remove the length attribute from all the reviews. Execute the following script to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qqoop4P-2t63"
   },
   "outputs": [],
   "source": [
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC5W_CeN29QJ"
   },
   "source": [
    "Once the reviews are sorted we will convert thed dataset so that it can be used to train TensorFlow 2.0 models. Run the following code to convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "h6N8nyLC2t1o"
   },
   "outputs": [],
   "source": [
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PsxcGPP3DCE"
   },
   "source": [
    "Finally, we can now pad our dataset for each batch. The batch size we are going to use is 32 which means that after processing 32 reviews, the weights of the neural network will be updated. To pad the reviews locally with respect to batches, execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "RD2tbGyT2tsp"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7M6pRFFl3E6j",
    "outputId": "db4633cf-1879-423c-8061-cd50e8373ae7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 5), dtype=int32, numpy=\n",
       " array([[    0,     0,     0,     0,     0],\n",
       "        [ 8711,     0,     0,     0,     0],\n",
       "        [ 3110, 20625,     0,     0,     0],\n",
       "        [ 7074,  8556,  8711,     0,     0],\n",
       "        [ 9042, 13141,  2336,     0,     0],\n",
       "        [ 1996, 10275, 11560,     0,     0],\n",
       "        [ 3492,  2172, 20625,     0,     0],\n",
       "        [ 2040,  2003, 13141,     0,     0],\n",
       "        [ 3319, 13141,  2375,     0,     0],\n",
       "        [ 2454,  2000,  1996,  9776,     0],\n",
       "        [ 2009,  2074,  3849, 20625,     0],\n",
       "        [ 3579,  2006,  1996, 11573,     0],\n",
       "        [ 6951,  7126,  5700,  8711,     0],\n",
       "        [ 2885,  5391, 16836, 19549,     0],\n",
       "        [ 3554,  2058,  2273,  2308,     0],\n",
       "        [ 8738,  2005,  2308,  2058,     0],\n",
       "        [16836, 19549,  2125,  4977,     0],\n",
       "        [ 2028,  2111,  2035,  7489,     0],\n",
       "        [ 3577, 26001, 16836,  1999,  7095],\n",
       "        [ 3361,  5375,  2000,  3532,  2945],\n",
       "        [20148,  3727,  2695,  2386, 11573],\n",
       "        [16169,  2000,  2495,  2003, 20625],\n",
       "        [ 8131,  2008,  2028,  5683, 20625],\n",
       "        [12052, 16836,  4727,  1999, 16738],\n",
       "        [ 2037, 13141,  3570,  3464, 15704],\n",
       "        [ 3082, 15811, 17552,  2945, 11573],\n",
       "        [25040, 11573,  4279, 16088,  6265],\n",
       "        [14754,  1999,  2342,  1997, 12992],\n",
       "        [ 2844,  2707,  2005,  2308,  3873],\n",
       "        [ 6735,  2008,  2081,  2068,  8711],\n",
       "        [ 2844,  7266,  2681,  2111, 11573],\n",
       "        [ 1049,  9776,  2025,  4039, 22953]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int32)>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCl8S0-x3VHg"
   },
   "source": [
    "#### seperate training and testing data - bit of a mad way to do it lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "UscZozHb3aSJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "PvY_eyqy3NmB"
   },
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "9UEMzWDp3miB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uSESMab3qmu"
   },
   "source": [
    "### creating model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot_11kpk3oxO"
   },
   "source": [
    "this is taken from [here](https://colab.research.google.com/drive/12noBxRkrZnIkHqvmdfFW2TGdOXFtNePM) . has 3 CNN layers. can also use LSTM layers instead and can also increase or decrease the number of layers (all this taken from the link I've been following [hereee](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "je8ztsRV3Nho"
   },
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9o9Jvz64Vzu"
   },
   "source": [
    "### values for model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pg4mC_kS3Nc_"
   },
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjqIkhEA4fQ3"
   },
   "source": [
    "not too sure what the craic is here lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "iekaNTQ74ttk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "hbEPvpgk4Mbw"
   },
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A30Y5HC441u2"
   },
   "source": [
    "Before we can actually train the model we need to compile it. The following script compiles the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Xa7NbILf43B5"
   },
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiEyujLd423e",
    "outputId": "83dd6606-0326-46fc-ecb6-6f85f50234c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 48s 154ms/step - loss: 0.3035 - accuracy: 0.9053\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.1659 - accuracy: 0.9358\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.0366 - accuracy: 0.9872\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 46s 152ms/step - loss: 0.0127 - accuracy: 0.9968\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 46s 152ms/step - loss: 0.0083 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f539666e350>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWkJCaxf5f7E"
   },
   "source": [
    "now to try the trained model on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRAx0Up55fx4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NqoPg5N4MTU",
    "outputId": "ff08547b-7199-4d10-d5ac-32d32498777a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 15ms/step - loss: 1.5106 - accuracy: 0.6335\n",
      "[1.5106418132781982, 0.6335227489471436]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_ZuJePs5mFI"
   },
   "source": [
    "### room for improvement - \n",
    "obvo fix up the code so its not plagiarised and its not for movie reviews.\n",
    "also try get a few diff models not just CNN\n",
    "try get it so that it's not just a binary check (e.g. you have the scale from 0 to 4 and not just 0 and 1)\n",
    "64% accuracy is a bit shit too tbh, especially when only 9% of them are patronizing would be wanting 91%+ for it to be better than random guessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvgUg_ZT5lWX",
    "outputId": "8c2ff258-269c-4929-9bb2-86b6e098f36e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09420834900338473"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2[df2[\"label\"]==1])/len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWjgW6tS3E2k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_CA2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
